{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read/Write functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(tar, fname):\n",
    "    member = tar.getmember(fname)\n",
    "    print(member.name)\n",
    "    tf = tar.extractfile(member)\n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in tf:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        (label,text) = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        data.append(text)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(tarfname):\n",
    "    \"\"\"Read the training and development data from the sentiment tar file.\n",
    "    The returned object contains various fields that store sentiment data, such as:\n",
    "\n",
    "    train_data,dev_data: array of documents (array of words)\n",
    "    train_fnames,dev_fnames: list of filenames of the doccuments (same length as data)\n",
    "    train_labels,dev_labels: the true string label for each document (same length as data)\n",
    "\n",
    "    The data is also preprocessed for use with scikit-learn, as:\n",
    "\n",
    "    count_vec: CountVectorizer used to process the data (for reapplication on new data)\n",
    "    trainX,devX: array of vectors representing Bags of Words, i.e. documents processed through the vectorizer\n",
    "    le: LabelEncoder, i.e. a mapper from string labels to ints (stored for reapplication)\n",
    "    target_labels: List of labels (same order as used in le)\n",
    "    trainy,devy: array of int labels, one for each document\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\")\n",
    "    trainname = \"train.tsv\"\n",
    "    devname = \"dev.tsv\"\n",
    "    for member in tar.getmembers():\n",
    "        if 'train.tsv' in member.name:\n",
    "            trainname = member.name\n",
    "        elif 'dev.tsv' in member.name:\n",
    "            devname = member.name\n",
    "                   \n",
    "    class Data: pass\n",
    "    sentiment = Data()\n",
    "    print(\"-- train data\")\n",
    "    sentiment.train_data, sentiment.train_labels = read_tsv(tar,trainname)\n",
    "    print(len(sentiment.train_data))\n",
    "\n",
    "    print(\"-- dev data\")\n",
    "    sentiment.dev_data, sentiment.dev_labels = read_tsv(tar, devname)\n",
    "    print(len(sentiment.dev_data))\n",
    "    print(\"-- transforming data and labels\")\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    sentiment.count_vect = CountVectorizer()\n",
    "    sentiment.trainX = sentiment.count_vect.fit_transform(sentiment.train_data)\n",
    "    sentiment.devX = sentiment.count_vect.transform(sentiment.dev_data)\n",
    "    from sklearn import preprocessing\n",
    "    sentiment.le = preprocessing.LabelEncoder()\n",
    "    sentiment.le.fit(sentiment.train_labels)\n",
    "    sentiment.target_labels = sentiment.le.classes_\n",
    "    sentiment.trainy = sentiment.le.transform(sentiment.train_labels)\n",
    "    sentiment.devy = sentiment.le.transform(sentiment.dev_labels)\n",
    "    tar.close()\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unlabeled(tarfname, sentiment):\n",
    "    \"\"\"Reads the unlabeled data.\n",
    "\n",
    "    The returned object contains three fields that represent the unlabeled data.\n",
    "\n",
    "    data: documents, represented as sequence of words\n",
    "    fnames: list of filenames, one for each document\n",
    "    X: bag of word vector for each document, using the sentiment.vectorizer\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\")\n",
    "    class Data: pass\n",
    "    unlabeled = Data()\n",
    "    unlabeled.data = []\n",
    "    \n",
    "    unlabeledname = \"unlabeled.tsv\"\n",
    "    for member in tar.getmembers():\n",
    "        if 'unlabeled.tsv' in member.name:\n",
    "            unlabeledname = member.name\n",
    "            \n",
    "    print(unlabeledname)\n",
    "    tf = tar.extractfile(unlabeledname)\n",
    "    for line in tf:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        text = line.strip()\n",
    "        unlabeled.data.append(text)\n",
    "        \n",
    "            \n",
    "    unlabeled.X = sentiment.count_vect.transform(unlabeled.data)\n",
    "    print(unlabeled.X.shape)\n",
    "    tar.close()\n",
    "    return unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords \n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-- train data\n",
      "sentiment/train.tsv\n",
      "4582\n",
      "-- dev data\n",
      "sentiment/dev.tsv\n",
      "458\n",
      "-- transforming data and labels\n",
      "\n",
      "Reading unlabeled data\n",
      "sentiment/unlabeled.tsv\n",
      "(91524, 9882)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "tarfname = \"data/sentiment.tar.gz\"\n",
    "sentiment = read_files(tarfname)\n",
    "print(\"\\nReading unlabeled data\")\n",
    "unlabeled = read_unlabeled(tarfname, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X, y, c):\n",
    "    \"\"\"Train a classifier using the given training data.\n",
    "\n",
    "    Trains logistic regression on the input data with default parameters.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    cls = LogisticRegression(C=c,random_state=0, solver='lbfgs', max_iter=10000)\n",
    "    cls.fit(X, y)\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, yt, cls, name='data'):\n",
    "    \"\"\"Evaluated a classifier on the given labeled data using accuracy.\"\"\"\n",
    "    from sklearn import metrics\n",
    "    yp = cls.predict(X)\n",
    "    acc = metrics.accuracy_score(yt, yp)\n",
    "    print(\"  Accuracy on %s  is: %s\" % (name, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset):\n",
    "    termsCount = defaultdict(int)\n",
    "    documentCount = defaultdict(int)\n",
    "\n",
    "    for d in dataset:\n",
    "        lower = ''.join([c for c in d.lower()])\n",
    "        r = re.split(r'\\W+', lower)\n",
    "        voc = set()\n",
    "        for i in range(len(r)):\n",
    "            w = r[i]\n",
    "            voc.add(w)\n",
    "            termsCount[w] += 1\n",
    "            \n",
    "            if i != len(r) - 1:\n",
    "                b = r[i] + ' ' + r[i + 1]\n",
    "                voc.add(b)\n",
    "                termsCount[b] += 1   \n",
    "            \n",
    "        for t in voc:\n",
    "            documentCount[t] += 1\n",
    "\n",
    "    terms = [t for t in termsCount]\n",
    "    termsId = dict(zip(terms, range(len(terms))))\n",
    "\n",
    "    idf = defaultdict(float)\n",
    "    for t in terms:\n",
    "        idf[t] = math.log(len(dataset) / (documentCount[t]))\n",
    "    return (termsId, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(document):\n",
    "    tf_table = defaultdict(int)\n",
    "    lower = ''.join([c for c in document.lower()])\n",
    "    r = re.split(r'\\W+', lower)\n",
    "    for i in range(len(r)):\n",
    "        w = r[i]\n",
    "        tf_table[w] += 1\n",
    "        \n",
    "        if i != len(r) - 1:\n",
    "            b = r[i] + ' ' + r[i + 1]\n",
    "            tf_table[b] += 1 \n",
    "        \n",
    "    for t in tf_table:\n",
    "        tf_table[t] = math.log(1 + tf_table[t])\n",
    "    return tf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix(dataset, termsId, idf):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    index = 0\n",
    "    for document in dataset:\n",
    "        tf_table = tf(document)\n",
    "        for t in tf_table:\n",
    "            if t in termsId:\n",
    "                row.append(index)\n",
    "                col.append(termsId[t])\n",
    "                data.append(tf_table[t] * idf[t])\n",
    "        index += 1\n",
    "    return csr_matrix((data, (row, col)), shape=(len(dataset), len(termsId)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsId, idf_table = tokenize(sentiment.train_data)\n",
    "trainX = tfidf_matrix(sentiment.train_data, wordsId, idf_table)\n",
    "devX = tfidf_matrix(sentiment.dev_data, wordsId, idf_table)\n",
    "cls = train_classifier(trainX, sentiment.trainy, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating\n",
      "  Accuracy on train  is: 1.0\n",
      "  Accuracy on dev  is: 0.8078602620087336\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating\")\n",
    "evaluate(trainX, sentiment.trainy, cls, 'train')\n",
    "evaluate(devX, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised TC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### most_confident_prediction( fix cutoff) + dev stop increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 1 : dev accuracy: 0.8078602620087336\n",
      "         train size:   4582\n",
      "loop 2 : dev accuracy: 0.8078602620087336\n",
      "         train size:   19602\n",
      "loop 3 : dev accuracy: 0.8013100436681223\n",
      "         train size:   34557\n",
      "loop 4 : dev accuracy: 0.7882096069868996\n",
      "         train size:   45763\n",
      "loop 5 : dev accuracy: 0.7903930131004366\n",
      "         train size:   52570\n",
      "loop 6 : dev accuracy: 0.7838427947598253\n",
      "         train size:   56310\n",
      "loop 7 : dev accuracy: 0.7838427947598253\n",
      "         train size:   58369\n",
      "loop 8 : dev accuracy: 0.7751091703056768\n",
      "         train size:   59611\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "cur_train_data = deepcopy(sentiment.train_data)\n",
    "cur_trainy = deepcopy(sentiment.trainy)\n",
    " \n",
    "unlabel_data = deepcopy(unlabeled.data)\n",
    "testX = unlabeled.X\n",
    "pre_acc = 0\n",
    "wordsId, idf_table_1 = tokenize(cur_train_data)\n",
    "index = 0\n",
    "while True:\n",
    "    index += 1\n",
    "    \n",
    "    wordsId, idf_table = tokenize(cur_train_data)\n",
    "    trainX = tfidf_matrix(cur_train_data, wordsId, idf_table)\n",
    "    devX = tfidf_matrix(sentiment.dev_data, wordsId, idf_table)\n",
    "    cls = train_classifier(trainX, cur_trainy, 0.1)\n",
    "    \n",
    "    dev_yp = cls.predict(devX)\n",
    "    dev_acc = metrics.accuracy_score(sentiment.devy, dev_yp)\n",
    "    \n",
    "    print(\"loop\", index, \": dev accuracy:\", dev_acc)\n",
    "    print(\"         train size:  \", len(cur_train_data))\n",
    "    if index == 8:\n",
    "        break\n",
    "    pre_acc = dev_acc\n",
    "        \n",
    "    testX = tfidf_matrix(unlabel_data, wordsId, idf_table)\n",
    "    test_yp = cls.predict(testX)\n",
    "    test_confidence = abs(cls.decision_function(testX))\n",
    "    \n",
    "    expand_data = []\n",
    "    expand_y = []    \n",
    "    \n",
    "    for i in range(len(test_confidence)):\n",
    "        if test_confidence[i] > 3.5:\n",
    "            expand_data.append(unlabeled.data[i])\n",
    "            expand_y.append(test_yp[i])\n",
    "    \n",
    "    cur_train_data = sentiment.train_data + expand_data\n",
    "    cur_trainy = np.concatenate((sentiment.trainy, np.array(expand_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_yp = cls.predict(devX)\n",
    "error_list = []\n",
    "for i in range(len(sentiment.dev_data)):\n",
    "    if dev_yp[i] != sentiment.devy[i]:\n",
    "        error_list.append((sentiment.dev_data[i], dev_yp[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
