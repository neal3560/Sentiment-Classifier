{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read/Write functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(tar, fname):\n",
    "    member = tar.getmember(fname)\n",
    "    print(member.name)\n",
    "    tf = tar.extractfile(member)\n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in tf:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        (label,text) = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        data.append(text)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(tarfname):\n",
    "    \"\"\"Read the training and development data from the sentiment tar file.\n",
    "    The returned object contains various fields that store sentiment data, such as:\n",
    "\n",
    "    train_data,dev_data: array of documents (array of words)\n",
    "    train_fnames,dev_fnames: list of filenames of the doccuments (same length as data)\n",
    "    train_labels,dev_labels: the true string label for each document (same length as data)\n",
    "\n",
    "    The data is also preprocessed for use with scikit-learn, as:\n",
    "\n",
    "    count_vec: CountVectorizer used to process the data (for reapplication on new data)\n",
    "    trainX,devX: array of vectors representing Bags of Words, i.e. documents processed through the vectorizer\n",
    "    le: LabelEncoder, i.e. a mapper from string labels to ints (stored for reapplication)\n",
    "    target_labels: List of labels (same order as used in le)\n",
    "    trainy,devy: array of int labels, one for each document\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\")\n",
    "    trainname = \"train.tsv\"\n",
    "    devname = \"dev.tsv\"\n",
    "    for member in tar.getmembers():\n",
    "        if 'train.tsv' in member.name:\n",
    "            trainname = member.name\n",
    "        elif 'dev.tsv' in member.name:\n",
    "            devname = member.name\n",
    "                   \n",
    "    class Data: pass\n",
    "    sentiment = Data()\n",
    "    print(\"-- train data\")\n",
    "    sentiment.train_data, sentiment.train_labels = read_tsv(tar,trainname)\n",
    "    print(len(sentiment.train_data))\n",
    "\n",
    "    print(\"-- dev data\")\n",
    "    sentiment.dev_data, sentiment.dev_labels = read_tsv(tar, devname)\n",
    "    print(len(sentiment.dev_data))\n",
    "    print(\"-- transforming data and labels\")\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    sentiment.count_vect = CountVectorizer()\n",
    "    sentiment.trainX = sentiment.count_vect.fit_transform(sentiment.train_data)\n",
    "    sentiment.devX = sentiment.count_vect.transform(sentiment.dev_data)\n",
    "    from sklearn import preprocessing\n",
    "    sentiment.le = preprocessing.LabelEncoder()\n",
    "    sentiment.le.fit(sentiment.train_labels)\n",
    "    sentiment.target_labels = sentiment.le.classes_\n",
    "    sentiment.trainy = sentiment.le.transform(sentiment.train_labels)\n",
    "    sentiment.devy = sentiment.le.transform(sentiment.dev_labels)\n",
    "    tar.close()\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unlabeled(tarfname, sentiment):\n",
    "    \"\"\"Reads the unlabeled data.\n",
    "\n",
    "    The returned object contains three fields that represent the unlabeled data.\n",
    "\n",
    "    data: documents, represented as sequence of words\n",
    "    fnames: list of filenames, one for each document\n",
    "    X: bag of word vector for each document, using the sentiment.vectorizer\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\")\n",
    "    class Data: pass\n",
    "    unlabeled = Data()\n",
    "    unlabeled.data = []\n",
    "    \n",
    "    unlabeledname = \"unlabeled.tsv\"\n",
    "    for member in tar.getmembers():\n",
    "        if 'unlabeled.tsv' in member.name:\n",
    "            unlabeledname = member.name\n",
    "            \n",
    "    print(unlabeledname)\n",
    "    tf = tar.extractfile(unlabeledname)\n",
    "    for line in tf:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        text = line.strip()\n",
    "        unlabeled.data.append(text)\n",
    "        \n",
    "            \n",
    "    unlabeled.X = sentiment.count_vect.transform(unlabeled.data)\n",
    "    print(unlabeled.X.shape)\n",
    "    tar.close()\n",
    "    return unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pred_kaggle_file(unlabeled, cls, outfname, sentiment):\n",
    "    \"\"\"Writes the predictions in Kaggle format.\n",
    "\n",
    "    Given the unlabeled object, classifier, outputfilename, and the sentiment object,\n",
    "    this function write sthe predictions of the classifier on the unlabeled data and\n",
    "    writes it to the outputfilename. The sentiment object is required to ensure\n",
    "    consistent label names.\n",
    "    \"\"\"\n",
    "    yp = cls.predict(unlabeled.X)\n",
    "    labels = sentiment.le.inverse_transform(yp)\n",
    "    f = open(outfname, 'w')\n",
    "    f.write(\"ID,LABEL\\n\")\n",
    "    for i in range(len(unlabeled.data)):\n",
    "        f.write(str(i+1))\n",
    "        f.write(\",\")\n",
    "        f.write(labels[i])\n",
    "        f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_basic_kaggle_file(tsvfile, outfname):\n",
    "    \"\"\"Writes the output Kaggle file of the naive baseline.\n",
    "\n",
    "    This baseline predicts POSITIVE for all the instances.\n",
    "    \"\"\"\n",
    "    f = open(outfname, 'w')\n",
    "    f.write(\"ID,LABEL\\n\")\n",
    "    i = 0\n",
    "    with open(tsvfile, 'r') as tf:\n",
    "        for line in tf:\n",
    "            (label,review) = line.strip().split(\"\\t\")\n",
    "            i += 1\n",
    "            f.write(str(i))\n",
    "            f.write(\",\")\n",
    "            f.write(\"POSITIVE\")\n",
    "            f.write(\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords \n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-- train data\n",
      "sentiment/train.tsv\n",
      "4582\n",
      "-- dev data\n",
      "sentiment/dev.tsv\n",
      "458\n",
      "-- transforming data and labels\n",
      "\n",
      "Reading unlabeled data\n",
      "sentiment/unlabeled.tsv\n",
      "(91524, 9882)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "tarfname = \"data/sentiment.tar.gz\"\n",
    "sentiment = read_files(tarfname)\n",
    "print(\"\\nReading unlabeled data\")\n",
    "unlabeled = read_unlabeled(tarfname, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X, y, c):\n",
    "    \"\"\"Train a classifier using the given training data.\n",
    "\n",
    "    Trains logistic regression on the input data with default parameters.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    cls = LogisticRegression(C=c,random_state=0, solver='lbfgs', max_iter=10000)\n",
    "    cls.fit(X, y)\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, yt, cls, name='data'):\n",
    "    \"\"\"Evaluated a classifier on the given labeled data using accuracy.\"\"\"\n",
    "    from sklearn import metrics\n",
    "    yp = cls.predict(X)\n",
    "    acc = metrics.accuracy_score(yt, yp)\n",
    "    print(\"  Accuracy on %s  is: %s\" % (name, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram BAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating\n",
      "  Accuracy on train  is: 0.9644260148406809\n",
      "  Accuracy on dev  is: 0.7816593886462883\n",
      "Writing predictions to a file\n"
     ]
    }
   ],
   "source": [
    "cls = train_classifier(sentiment.trainX, sentiment.trainy, 0.6)\n",
    "print(\"\\nEvaluating\")\n",
    "evaluate(sentiment.trainX, sentiment.trainy, cls, 'train')\n",
    "evaluate(sentiment.devX, sentiment.devy, cls, 'dev')\n",
    "\n",
    "print(\"Writing predictions to a file\")\n",
    "unlabeled.X = sentiment.count_vect.transform(unlabeled.data)\n",
    "write_pred_kaggle_file(unlabeled, cls, \"data/sentiment-pred.csv\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(document):\n",
    "    tf_table = defaultdict(int)\n",
    "    lower = ''.join([c for c in document.lower()])\n",
    "    r = re.split(r'\\W+', lower)\n",
    "    for w in r:\n",
    "        tf_table[w] += 1\n",
    "    for t in tf_table:\n",
    "        tf_table[t] = math.log(1 + tf_table[t])\n",
    "    return tf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix(dataset):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    index = 0\n",
    "    for document in dataset:\n",
    "        tf_table = tf(document)\n",
    "        for w in tf_table:\n",
    "            if w in wordsId:\n",
    "                row.append(index)\n",
    "                col.append(wordsId[w])\n",
    "                data.append(tf_table[w] * idf[w])\n",
    "        index += 1\n",
    "    return csr_matrix((data, (row, col)), shape=(len(dataset), len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = set('0 1 2 3 4 5 6 7 8 9'.split())\n",
    "wordsCount = defaultdict(int)\n",
    "documentCount = defaultdict(int) \n",
    "\n",
    "for d in sentiment.train_data:\n",
    "    lower = ''.join([c for c in d.lower()])\n",
    "    r = re.split(r'\\W+', lower)\n",
    "    for w in r:\n",
    "        wordsCount[w] += 1\n",
    "\n",
    "    for w in set(r):\n",
    "        documentCount[w] += 1\n",
    "\n",
    "words = [x for x in wordsCount]\n",
    "wordsId = dict(zip(words, range(len(words))))\n",
    "\n",
    "idf = defaultdict(float)\n",
    "for w in words:\n",
    "    idf[w] = math.log(len(sentiment.train_data) / documentCount[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_trainX = tfidf_matrix(sentiment.train_data)\n",
    "uni_devX = tfidf_matrix(sentiment.dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating\n",
      "  Accuracy on train  is: 0.9997817546922741\n",
      "  Accuracy on dev  is: 0.7969432314410481\n"
     ]
    }
   ],
   "source": [
    "cls = train_classifier(uni_trainX, sentiment.trainy, 0.7)\n",
    "print(\"\\nEvaluating\")\n",
    "evaluate(uni_trainX, sentiment.trainy, cls, 'train')\n",
    "evaluate(uni_devX, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictions to a file\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing predictions to a file\")\n",
    "unlabeled.X = tfidf_matrix(unlabeled.data)\n",
    "write_pred_kaggle_file(unlabeled, cls, \"data/sentiment-pred.csv\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_bigram(document):\n",
    "    tf_table = defaultdict(int)\n",
    "    lower = ''.join([c for c in document.lower()])\n",
    "    r = re.split(r'\\W+', lower)\n",
    "    for i in range(len(r) - 1):\n",
    "        b = r[i] + ' ' + r[i + 1]\n",
    "        tf_table[b] += 1\n",
    "    for t in tf_table:\n",
    "        tf_table[t] = math.log(1 + tf_table[t])\n",
    "    return tf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix_bigram(dataset):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    index = 0\n",
    "    for document in dataset:\n",
    "        tf_table = tf_bigram(document)\n",
    "        for b in tf_table:\n",
    "            if b in bigramsId:\n",
    "                row.append(index)\n",
    "                col.append(bigramsId[b])\n",
    "                data.append(tf_table[b] * bigram_idf[b])\n",
    "        index += 1\n",
    "    return csr_matrix((data, (row, col)), shape=(len(dataset), len(bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramCount = defaultdict(int)\n",
    "bigramDocumentCount = defaultdict(int)\n",
    "\n",
    "for d in sentiment.train_data:\n",
    "    lower = ''.join([c for c in d.lower()])\n",
    "    r = re.split(r'\\W+', lower)\n",
    "    br = set()\n",
    "    for i in range(len(r) - 1):\n",
    "        b = r[i] + ' ' + r[i + 1]\n",
    "        br.add(b)\n",
    "        bigramCount[b] += 1\n",
    "    for b in br:\n",
    "        bigramDocumentCount[b] += 1\n",
    "\n",
    "bigrams = [b for b in bigramCount]\n",
    "bigramsId = dict(zip(bigrams, range(len(bigrams))))\n",
    "\n",
    "bigram_idf = defaultdict(float)\n",
    "for b in bigrams:\n",
    "    bigram_idf[b] = math.log(len(sentiment.train_data) / bigramDocumentCount[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_trainX = tfidf_matrix_bigram(sentiment.train_data)\n",
    "bi_devX = tfidf_matrix_bigram(sentiment.dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating\n",
      "  Accuracy on train  is: 0.9993452640768223\n",
      "  Accuracy on dev  is: 0.7860262008733624\n"
     ]
    }
   ],
   "source": [
    "cls = train_classifier(bi_trainX, sentiment.trainy, 0.006)\n",
    "print(\"\\nEvaluating\")\n",
    "evaluate(bi_trainX, sentiment.trainy, cls, 'train')\n",
    "evaluate(bi_devX, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "ubi_trainX = hstack((uni_trainX, bi_trainX))\n",
    "ubi_devX = hstack((uni_devX, bi_devX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating\n",
      "  Accuracy on train  is: 1.0\n",
      "  Accuracy on dev  is: 0.8078602620087336\n"
     ]
    }
   ],
   "source": [
    "cls = train_classifier(ubi_trainX, sentiment.trainy, 0.1)\n",
    "print(\"\\nEvaluating\")\n",
    "evaluate(ubi_trainX, sentiment.trainy, cls, 'train')\n",
    "evaluate(ubi_devX, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictions to a file\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing predictions to a file\")\n",
    "unlabeled.X = hstack((tfidf_matrix(unlabeled.data), tfidf_matrix_bigram(unlabeled.data)))\n",
    "write_pred_kaggle_file(unlabeled, cls, \"data/sentiment-pred.csv\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset):\n",
    "    termsCount = defaultdict(int)\n",
    "    documentCount = defaultdict(int)\n",
    "\n",
    "    for d in dataset:\n",
    "        lower = ''.join([c for c in d.lower()])\n",
    "        r = re.split(r'\\W+', lower)\n",
    "        voc = set()\n",
    "        for i in range(len(r)):\n",
    "            w = r[i]\n",
    "            voc.add(w)\n",
    "            termsCount[w] += 1\n",
    "            \n",
    "            if i != len(r) - 1:\n",
    "                b = r[i] + ' ' + r[i + 1]\n",
    "                voc.add(b)\n",
    "                termsCount[b] += 1\n",
    "            \n",
    "            \n",
    "        for t in voc:\n",
    "            documentCount[t] += 1\n",
    "\n",
    "    terms = [t for t in termsCount]\n",
    "    termsId = dict(zip(terms, range(len(terms))))\n",
    "\n",
    "    idf = defaultdict(float)\n",
    "    for t in terms:\n",
    "        idf[t] = math.log(len(dataset) / (documentCount[t]))\n",
    "    return (termsId, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(document):\n",
    "    tf_table = defaultdict(int)\n",
    "    lower = ''.join([c for c in document.lower()])\n",
    "    r = re.split(r'\\W+', lower)\n",
    "    for i in range(len(r)):\n",
    "        w = r[i]\n",
    "        tf_table[w] += 1\n",
    "        \n",
    "        if i != len(r) - 1:\n",
    "            b = r[i] + ' ' + r[i + 1]\n",
    "            tf_table[b] += 1 \n",
    "        \n",
    "    for t in tf_table:\n",
    "        tf_table[t] = math.log(1 + tf_table[t])\n",
    "    return tf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix(dataset, termsId, idf):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    index = 0\n",
    "    for document in dataset:\n",
    "        tf_table = tf(document)\n",
    "        for t in tf_table:\n",
    "            if t in termsId:\n",
    "                row.append(index)\n",
    "                col.append(termsId[t])\n",
    "                data.append(tf_table[t] * idf[t])\n",
    "        index += 1\n",
    "    return csr_matrix((data, (row, col)), shape=(len(dataset), len(termsId)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsId, idf_table_1 = tokenize(sentiment.train_data)\n",
    "diff_list = []\n",
    "for w in idf_table_1:\n",
    "    diff = abs(idf_table[w] - idf_table_1[w])\n",
    "    diff_list.append((diff, w))\n",
    "diff_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.5657045367685942, '魚肉也算新鮮 美中不足是壽司飯煮的不是很透'),\n",
       " (2.5657045367685942, '魚肉也算新鮮'),\n",
       " (2.5657045367685942, '飯給吃完 要不然就要算單點價錢基於食物與服務我只能給它兩星級'),\n",
       " (2.5657045367685942, '飯給吃完'),\n",
       " (2.5657045367685942, '還是堅持要肉 飯給吃完'),\n",
       " (2.5657045367685942, '還是堅持要肉'),\n",
       " (2.5657045367685942, '要不然就要算單點價錢基於食物與服務我只能給它兩星級 5'),\n",
       " (2.5657045367685942, '要不然就要算單點價錢基於食物與服務我只能給它兩星級'),\n",
       " (2.5657045367685942, '美中不足是壽司飯煮的不是很透 反應給日本師傅與經理後'),\n",
       " (2.5657045367685942, '美中不足是壽司飯煮的不是很透'),\n",
       " (2.5657045367685942, '昨天到這家口碑十足的日式壽司店 魚肉也算新鮮'),\n",
       " (2.5657045367685942, '昨天到這家口碑十足的日式壽司店'),\n",
       " (2.5657045367685942, '年後 昨天到這家口碑十足的日式壽司店'),\n",
       " (2.5657045367685942, '年後'),\n",
       " (2.5657045367685942, '反應給日本師傅與經理後 還是堅持要肉'),\n",
       " (2.5657045367685942, '反應給日本師傅與經理後'),\n",
       " (2.5657045367685942, 'über 25'),\n",
       " (2.5657045367685942, 'über'),\n",
       " (2.5657045367685942, 'être reçus'),\n",
       " (2.5657045367685942, 'été pour'),\n",
       " (2.5657045367685942, 'étions assises'),\n",
       " (2.5657045367685942, 'étions'),\n",
       " (2.5657045367685942, 'était pas'),\n",
       " (2.5657045367685942, 'équipe distante'),\n",
       " (2.5657045367685942, 'élaborée dans'),\n",
       " (2.5657045367685942, 'élaborée'),\n",
       " (2.5657045367685942, 'édition de'),\n",
       " (2.5657045367685942, 'édition'),\n",
       " (2.5657045367685942, 'ça n'),\n",
       " (2.5657045367685942, 'âgée mais'),\n",
       " (2.5657045367685942, 'âgée'),\n",
       " (2.5657045367685942, 'à une'),\n",
       " (2.5657045367685942, 'à te'),\n",
       " (2.5657045367685942, 'à table'),\n",
       " (2.5657045367685942, 'à sa'),\n",
       " (2.5657045367685942, 'à outremont'),\n",
       " (2.5657045367685942, 'à mes'),\n",
       " (2.5657045367685942, 'à faire'),\n",
       " (2.5657045367685942, 'à 10'),\n",
       " (2.5657045367685942, 'zwar fast'),\n",
       " (2.5657045367685942, 'zwar'),\n",
       " (2.5657045367685942, 'zutaten wurde'),\n",
       " (2.5657045367685942, 'zutaten befanden'),\n",
       " (2.5657045367685942, 'zutaten'),\n",
       " (2.5657045367685942, 'zusammen gutes'),\n",
       " (2.5657045367685942, 'zusammen'),\n",
       " (2.5657045367685942, 'zupas before'),\n",
       " (2.5657045367685942, 'zunächst etwas'),\n",
       " (2.5657045367685942, 'zunächst'),\n",
       " (2.5657045367685942, 'zum ersten'),\n",
       " (2.5657045367685942, 'zugewiesen uns'),\n",
       " (2.5657045367685942, 'zugewiesen'),\n",
       " (2.5657045367685942, 'zucchini seriously'),\n",
       " (2.5657045367685942, 'zucchini rounds'),\n",
       " (2.5657045367685942, 'zucchini in'),\n",
       " (2.5657045367685942, 'zucchini curry'),\n",
       " (2.5657045367685942, 'zu schweigen'),\n",
       " (2.5657045367685942, 'zoo nor'),\n",
       " (2.5657045367685942, 'zone seating'),\n",
       " (2.5657045367685942, 'zoltan was'),\n",
       " (2.5657045367685942, 'zoltan'),\n",
       " (2.5657045367685942, 'ziti with'),\n",
       " (2.5657045367685942, 'ziti etc'),\n",
       " (2.5657045367685942, 'zipps and'),\n",
       " (2.5657045367685942, 'zipping up'),\n",
       " (2.5657045367685942, 'zipping'),\n",
       " (2.5657045367685942, 'zippered pullovers'),\n",
       " (2.5657045367685942, 'zippered'),\n",
       " (2.5657045367685942, 'zipper had'),\n",
       " (2.5657045367685942, 'zip into'),\n",
       " (2.5657045367685942, 'zing first'),\n",
       " (2.5657045367685942, 'zinelle did'),\n",
       " (2.5657045367685942, 'zinelle'),\n",
       " (2.5657045367685942, 'zine'),\n",
       " (2.5657045367685942, 'zinburger so'),\n",
       " (2.5657045367685942, 'zimmern zugewiesen'),\n",
       " (2.5657045367685942, 'zillion times'),\n",
       " (2.5657045367685942, 'zero that'),\n",
       " (2.5657045367685942, 'zero genuine'),\n",
       " (2.5657045367685942, 'zero do'),\n",
       " (2.5657045367685942, 'zero atmosphere'),\n",
       " (2.5657045367685942, 'zero apologies'),\n",
       " (2.5657045367685942, 'zach was'),\n",
       " (2.5657045367685942, 'yummy yes'),\n",
       " (2.5657045367685942, 'yummy when'),\n",
       " (2.5657045367685942, 'yummy wanted'),\n",
       " (2.5657045367685942, 'yummy tacos'),\n",
       " (2.5657045367685942, 'yummy pastries'),\n",
       " (2.5657045367685942, 'yummy moscato'),\n",
       " (2.5657045367685942, 'yummy illy'),\n",
       " (2.5657045367685942, 'yummy fried'),\n",
       " (2.5657045367685942, 'yummy concrete'),\n",
       " (2.5657045367685942, 'yummy ayce'),\n",
       " (2.5657045367685942, 'yummmmmm '),\n",
       " (2.5657045367685942, 'yummmmmm'),\n",
       " (2.5657045367685942, 'yummmm i'),\n",
       " (2.5657045367685942, 'yummmm 2'),\n",
       " (2.5657045367685942, 'yumm not'),\n",
       " (2.5657045367685942, 'yumm it'),\n",
       " (2.5657045367685942, 'yum will'),\n",
       " (2.5657045367685942, 'yum went'),\n",
       " (2.5657045367685942, 'yum they'),\n",
       " (2.5657045367685942, 'yum candied'),\n",
       " (2.5657045367685942, 'yum amazing'),\n",
       " (2.5657045367685942, 'yuk '),\n",
       " (2.5657045367685942, 'yucky they'),\n",
       " (2.5657045367685942, 'yucky that'),\n",
       " (2.5657045367685942, 'yuck be'),\n",
       " (2.5657045367685942, 'youtu be'),\n",
       " (2.5657045367685942, 'youtu'),\n",
       " (2.5657045367685942, 'yourself yogurt'),\n",
       " (2.5657045367685942, 'yourself with'),\n",
       " (2.5657045367685942, 'yourself food'),\n",
       " (2.5657045367685942, 'yourself ac'),\n",
       " (2.5657045367685942, 'youre looking'),\n",
       " (2.5657045367685942, 'your weight'),\n",
       " (2.5657045367685942, 'your vanilla'),\n",
       " (2.5657045367685942, 'your techies'),\n",
       " (2.5657045367685942, 'your sorry'),\n",
       " (2.5657045367685942, 'your so'),\n",
       " (2.5657045367685942, 'your seizure'),\n",
       " (2.5657045367685942, 'your roast'),\n",
       " (2.5657045367685942, 'your rich'),\n",
       " (2.5657045367685942, 'your rent'),\n",
       " (2.5657045367685942, 'your register'),\n",
       " (2.5657045367685942, 'your point'),\n",
       " (2.5657045367685942, 'your mom'),\n",
       " (2.5657045367685942, 'your lips'),\n",
       " (2.5657045367685942, 'your larger'),\n",
       " (2.5657045367685942, 'your language'),\n",
       " (2.5657045367685942, 'your idea'),\n",
       " (2.5657045367685942, 'your horizon'),\n",
       " (2.5657045367685942, 'your grandma'),\n",
       " (2.5657045367685942, 'your glitches'),\n",
       " (2.5657045367685942, 'your gf'),\n",
       " (2.5657045367685942, 'your faux'),\n",
       " (2.5657045367685942, 'your fancy'),\n",
       " (2.5657045367685942, 'your faded'),\n",
       " (2.5657045367685942, 'your faculty'),\n",
       " (2.5657045367685942, 'your entry'),\n",
       " (2.5657045367685942, 'your curry'),\n",
       " (2.5657045367685942, 'your clothing'),\n",
       " (2.5657045367685942, 'your classy'),\n",
       " (2.5657045367685942, 'your city'),\n",
       " (2.5657045367685942, 'your bread'),\n",
       " (2.5657045367685942, 'your being'),\n",
       " (2.5657045367685942, 'your avarage'),\n",
       " (2.5657045367685942, 'your actual'),\n",
       " (2.5657045367685942, 'your 7pm'),\n",
       " (2.5657045367685942, 'younger kids'),\n",
       " (2.5657045367685942, 'young vietnamese'),\n",
       " (2.5657045367685942, 'young untrained'),\n",
       " (2.5657045367685942, 'young type'),\n",
       " (2.5657045367685942, 'young things'),\n",
       " (2.5657045367685942, 'young single'),\n",
       " (2.5657045367685942, 'young hippie'),\n",
       " (2.5657045367685942, 'young family'),\n",
       " (2.5657045367685942, 'young dumb'),\n",
       " (2.5657045367685942, 'young cook'),\n",
       " (2.5657045367685942, 'youi tried'),\n",
       " (2.5657045367685942, 'youi had'),\n",
       " (2.5657045367685942, 'youi'),\n",
       " (2.5657045367685942, 'youevery time'),\n",
       " (2.5657045367685942, 'youevery'),\n",
       " (2.5657045367685942, 'you yeah'),\n",
       " (2.5657045367685942, 'you wine'),\n",
       " (2.5657045367685942, 'you watch'),\n",
       " (2.5657045367685942, 'you uh'),\n",
       " (2.5657045367685942, 'you tuesday'),\n",
       " (2.5657045367685942, 'you trade'),\n",
       " (2.5657045367685942, 'you time'),\n",
       " (2.5657045367685942, 'you ten'),\n",
       " (2.5657045367685942, 'you sweat'),\n",
       " (2.5657045367685942, 'you stupid'),\n",
       " (2.5657045367685942, 'you steered'),\n",
       " (2.5657045367685942, 'you standing'),\n",
       " (2.5657045367685942, 'you sprout'),\n",
       " (2.5657045367685942, 'you sooner'),\n",
       " (2.5657045367685942, 'you snapped'),\n",
       " (2.5657045367685942, 'you shay'),\n",
       " (2.5657045367685942, 'you saw'),\n",
       " (2.5657045367685942, 'you saved'),\n",
       " (2.5657045367685942, 'you satisfy'),\n",
       " (2.5657045367685942, 'you rue'),\n",
       " (2.5657045367685942, 'you reserved'),\n",
       " (2.5657045367685942, 'you received'),\n",
       " (2.5657045367685942, 'you pump'),\n",
       " (2.5657045367685942, 'you pray'),\n",
       " (2.5657045367685942, 'you practically'),\n",
       " (2.5657045367685942, 'you points'),\n",
       " (2.5657045367685942, 'you nail'),\n",
       " (2.5657045367685942, 'you mean'),\n",
       " (2.5657045367685942, 'you maybe'),\n",
       " (2.5657045367685942, 'you knowing'),\n",
       " (2.5657045367685942, 'you john'),\n",
       " (2.5657045367685942, 'you instagram'),\n",
       " (2.5657045367685942, 'you helped'),\n",
       " (2.5657045367685942, 'you hanging'),\n",
       " (2.5657045367685942, 'you getting'),\n",
       " (2.5657045367685942, 'you everything'),\n",
       " (2.5657045367685942, 'you entering'),\n",
       " (2.5657045367685942, 'you entered'),\n",
       " (2.5657045367685942, 'you designed'),\n",
       " (2.5657045367685942, 'you denise'),\n",
       " (2.5657045367685942, 'you choosde'),\n",
       " (2.5657045367685942, 'you china'),\n",
       " (2.5657045367685942, 'you chicken'),\n",
       " (2.5657045367685942, 'you checkout'),\n",
       " (2.5657045367685942, 'you charging'),\n",
       " (2.5657045367685942, 'you booty'),\n",
       " (2.5657045367685942, 'you attempt'),\n",
       " (2.5657045367685942, 'you asking'),\n",
       " (2.5657045367685942, 'you allll'),\n",
       " (2.5657045367685942, 'you 530'),\n",
       " (2.5657045367685942, 'you 30'),\n",
       " (2.5657045367685942, 'yorker to'),\n",
       " (2.5657045367685942, 'yorker having'),\n",
       " (2.5657045367685942, 'yorker at'),\n",
       " (2.5657045367685942, 'york steamer'),\n",
       " (2.5657045367685942, 'york spot'),\n",
       " (2.5657045367685942, 'york so'),\n",
       " (2.5657045367685942, 'york from'),\n",
       " (2.5657045367685942, 'yoooooooou ooooofor'),\n",
       " (2.5657045367685942, 'yoooooooou'),\n",
       " (2.5657045367685942, 'yogurtology yogurt'),\n",
       " (2.5657045367685942, 'yogurt this'),\n",
       " (2.5657045367685942, 'yogurt salad'),\n",
       " (2.5657045367685942, 'yogurt quality'),\n",
       " (2.5657045367685942, 'yogurt out'),\n",
       " (2.5657045367685942, 'yogurt itself'),\n",
       " (2.5657045367685942, 'yogurt drink'),\n",
       " (2.5657045367685942, 'yogurt craze'),\n",
       " (2.5657045367685942, 'yogurt as'),\n",
       " (2.5657045367685942, 'yoginis i'),\n",
       " (2.5657045367685942, 'yoginis'),\n",
       " (2.5657045367685942, 'yogini and'),\n",
       " (2.5657045367685942, 'yogini'),\n",
       " (2.5657045367685942, 'yoga tuesdays'),\n",
       " (2.5657045367685942, 'yoga they'),\n",
       " (2.5657045367685942, 'yoga teacher'),\n",
       " (2.5657045367685942, 'yoga on'),\n",
       " (2.5657045367685942, 'yoga nirvana'),\n",
       " (2.5657045367685942, 'yoga instructor'),\n",
       " (2.5657045367685942, 'yoga home'),\n",
       " (2.5657045367685942, 'yoga experience'),\n",
       " (2.5657045367685942, 'yoda too'),\n",
       " (2.5657045367685942, 'yoda'),\n",
       " (2.5657045367685942, 'yo sexy'),\n",
       " (2.5657045367685942, 'yinzers from'),\n",
       " (2.5657045367685942, 'yinzers'),\n",
       " (2.5657045367685942, 'yikes where'),\n",
       " (2.5657045367685942, 'yikes how'),\n",
       " (2.5657045367685942, 'yew fantastic'),\n",
       " (2.5657045367685942, 'yeux plus'),\n",
       " (2.5657045367685942, 'yeux'),\n",
       " (2.5657045367685942, 'yet unique'),\n",
       " (2.5657045367685942, 'yet she'),\n",
       " (2.5657045367685942, 'yet professional'),\n",
       " (2.5657045367685942, 'yet loud'),\n",
       " (2.5657045367685942, 'yet knocks'),\n",
       " (2.5657045367685942, 'yet higher'),\n",
       " (2.5657045367685942, 'yesterday service'),\n",
       " (2.5657045367685942, 'yesterday september'),\n",
       " (2.5657045367685942, 'yesterday louella'),\n",
       " (2.5657045367685942, 'yesterday he'),\n",
       " (2.5657045367685942, 'yesterday from'),\n",
       " (2.5657045367685942, 'yesterday during'),\n",
       " (2.5657045367685942, 'yesterday don'),\n",
       " (2.5657045367685942, 'yesterday bland'),\n",
       " (2.5657045367685942, 'yesterday 10'),\n",
       " (2.5657045367685942, 'yess it'),\n",
       " (2.5657045367685942, 'yess'),\n",
       " (2.5657045367685942, 'yes service'),\n",
       " (2.5657045367685942, 'yes pudding'),\n",
       " (2.5657045367685942, 'yes organic'),\n",
       " (2.5657045367685942, 'yes amazing'),\n",
       " (2.5657045367685942, 'yep screwed'),\n",
       " (2.5657045367685942, 'yelpers voted'),\n",
       " (2.5657045367685942, 'yelpers here'),\n",
       " (2.5657045367685942, 'yelper my'),\n",
       " (2.5657045367685942, 'yelper ingrid'),\n",
       " (2.5657045367685942, 'yelped it'),\n",
       " (2.5657045367685942, 'yelp went'),\n",
       " (2.5657045367685942, 'yelp very'),\n",
       " (2.5657045367685942, 'yelp user'),\n",
       " (2.5657045367685942, 'yelp thus'),\n",
       " (2.5657045367685942, 'yelp simply'),\n",
       " (2.5657045367685942, 'yelp science'),\n",
       " (2.5657045367685942, 'yelp raised'),\n",
       " (2.5657045367685942, 'yelp prompt'),\n",
       " (2.5657045367685942, 'yelp only'),\n",
       " (2.5657045367685942, 'yelp many'),\n",
       " (2.5657045367685942, 'yelp making'),\n",
       " (2.5657045367685942, 'yelp knows'),\n",
       " (2.5657045367685942, 'yelp kinkos'),\n",
       " (2.5657045367685942, 'yelp instead'),\n",
       " (2.5657045367685942, 'yelp homepage'),\n",
       " (2.5657045367685942, 'yelp cool'),\n",
       " (2.5657045367685942, 'yelp allow'),\n",
       " (2.5657045367685942, 'yelp 10'),\n",
       " (2.5657045367685942, 'yellowtail for'),\n",
       " (2.5657045367685942, 'yellowtail an'),\n",
       " (2.5657045367685942, 'yelling names'),\n",
       " (2.5657045367685942, 'yelling about'),\n",
       " (2.5657045367685942, 'years styles'),\n",
       " (2.5657045367685942, 'years starting'),\n",
       " (2.5657045367685942, 'years serving'),\n",
       " (2.5657045367685942, 'years seated'),\n",
       " (2.5657045367685942, 'years roach'),\n",
       " (2.5657045367685942, 'years our'),\n",
       " (2.5657045367685942, 'years nick'),\n",
       " (2.5657045367685942, 'years most'),\n",
       " (2.5657045367685942, 'years lots'),\n",
       " (2.5657045367685942, 'years james'),\n",
       " (2.5657045367685942, 'years ginger'),\n",
       " (2.5657045367685942, 'years friday'),\n",
       " (2.5657045367685942, 'year yes'),\n",
       " (2.5657045367685942, 'year which'),\n",
       " (2.5657045367685942, 'year veteran'),\n",
       " (2.5657045367685942, 'year trip'),\n",
       " (2.5657045367685942, 'year spent'),\n",
       " (2.5657045367685942, 'year some'),\n",
       " (2.5657045367685942, 'year same'),\n",
       " (2.5657045367685942, 'year ordered'),\n",
       " (2.5657045367685942, 'year may'),\n",
       " (2.5657045367685942, 'year june'),\n",
       " (2.5657045367685942, 'year here'),\n",
       " (2.5657045367685942, 'year assuming'),\n",
       " (2.5657045367685942, 'year 2015'),\n",
       " (2.5657045367685942, 'year 2014'),\n",
       " (2.5657045367685942, 'yeah thanks'),\n",
       " (2.5657045367685942, 'yeah every'),\n",
       " (2.5657045367685942, 'yeah do'),\n",
       " (2.5657045367685942, 'yeah 30'),\n",
       " (2.5657045367685942, 'yeaaa the'),\n",
       " (2.5657045367685942, 'yeaaa'),\n",
       " (2.5657045367685942, 'yea so'),\n",
       " (2.5657045367685942, 'yea for'),\n",
       " (2.5657045367685942, 'ye s'),\n",
       " (2.5657045367685942, 'ye'),\n",
       " (2.5657045367685942, 'yay sushi'),\n",
       " (2.5657045367685942, 'yay and'),\n",
       " (2.5657045367685942, 'yarn michael'),\n",
       " (2.5657045367685942, 'yards to'),\n",
       " (2.5657045367685942, 'yards staff'),\n",
       " (2.5657045367685942, 'yard when'),\n",
       " (2.5657045367685942, 'yard there'),\n",
       " (2.5657045367685942, 'yard the'),\n",
       " (2.5657045367685942, 'yard grass'),\n",
       " (2.5657045367685942, 'yard cleanup'),\n",
       " (2.5657045367685942, 'yard clean'),\n",
       " (2.5657045367685942, 'yard '),\n",
       " (2.5657045367685942, 'yahoo mail'),\n",
       " (2.5657045367685942, 'ya knowyou'),\n",
       " (2.5657045367685942, 'ya canned'),\n",
       " (2.5657045367685942, 'y vais'),\n",
       " (2.5657045367685942, 'y mar'),\n",
       " (2.5657045367685942, 'y feel'),\n",
       " (2.5657045367685942, 'y described'),\n",
       " (2.5657045367685942, 'y chang'),\n",
       " (2.5657045367685942, 'xtreme manufacturing'),\n",
       " (2.5657045367685942, 'xrays kept'),\n",
       " (2.5657045367685942, 'xoxoxo '),\n",
       " (2.5657045367685942, 'xl pizza'),\n",
       " (2.5657045367685942, 'xl'),\n",
       " (2.5657045367685942, 'xile to'),\n",
       " (2.5657045367685942, 'xile'),\n",
       " (2.5657045367685942, 'xeo pan'),\n",
       " (2.5657045367685942, 'xavier was'),\n",
       " (2.5657045367685942, 'x was'),\n",
       " (2.5657045367685942, 'wynn while'),\n",
       " (2.5657045367685942, 'wynn instead'),\n",
       " (2.5657045367685942, 'wyndham that'),\n",
       " (2.5657045367685942, 'wurden sehr'),\n",
       " (2.5657045367685942, 'wurden'),\n",
       " (2.5657045367685942, 'wurde sehr'),\n",
       " (2.5657045367685942, 'wurde nicht'),\n",
       " (2.5657045367685942, 'wurde ich'),\n",
       " (2.5657045367685942, 'wurde alles'),\n",
       " (2.5657045367685942, 'wurde'),\n",
       " (2.5657045367685942, 'wtf terrible'),\n",
       " (2.5657045367685942, 'wrong yet'),\n",
       " (2.5657045367685942, 'wrong three'),\n",
       " (2.5657045367685942, 'wrong sheraton'),\n",
       " (2.5657045367685942, 'wrong parts'),\n",
       " (2.5657045367685942, 'wrong lotus'),\n",
       " (2.5657045367685942, 'wrong looking'),\n",
       " (2.5657045367685942, 'wrong got'),\n",
       " (2.5657045367685942, 'wrong genders'),\n",
       " (2.5657045367685942, 'wrong close'),\n",
       " (2.5657045367685942, 'written something'),\n",
       " (2.5657045367685942, 'written signs'),\n",
       " (2.5657045367685942, 'writing out'),\n",
       " (2.5657045367685942, 'writing multiple'),\n",
       " (2.5657045367685942, 'writing contracts'),\n",
       " (2.5657045367685942, 'write and'),\n",
       " (2.5657045367685942, 'wrf989sdae for'),\n",
       " (2.5657045367685942, 'wrf989sdae'),\n",
       " (2.5657045367685942, 'wrench the'),\n",
       " (2.5657045367685942, 'wrench'),\n",
       " (2.5657045367685942, 'wraps sandwiches'),\n",
       " (2.5657045367685942, 'wrapper the'),\n",
       " (2.5657045367685942, 'wrapper same'),\n",
       " (2.5657045367685942, 'wrapper s'),\n",
       " (2.5657045367685942, 'wrapper and'),\n",
       " (2.5657045367685942, 'wrapped hot'),\n",
       " (2.5657045367685942, 'wrap ___'),\n",
       " (2.5657045367685942, 'wranglers they'),\n",
       " (2.5657045367685942, 'wranglers games'),\n",
       " (2.5657045367685942, 'wranglers'),\n",
       " (2.5657045367685942, 'wracking at'),\n",
       " (2.5657045367685942, 'wow yes'),\n",
       " (2.5657045367685942, 'wow watched'),\n",
       " (2.5657045367685942, 'wow sh'),\n",
       " (2.5657045367685942, 'wow he'),\n",
       " (2.5657045367685942, 'wow beautiful'),\n",
       " (2.5657045367685942, 'wow 11'),\n",
       " (2.5657045367685942, 'wouldnt recommend'),\n",
       " (2.5657045367685942, 'woulda started'),\n",
       " (2.5657045367685942, 'would willingly'),\n",
       " (2.5657045367685942, 'would why'),\n",
       " (2.5657045367685942, 'would who'),\n",
       " (2.5657045367685942, 'would trust'),\n",
       " (2.5657045367685942, 'would that'),\n",
       " (2.5657045367685942, 'would stock'),\n",
       " (2.5657045367685942, 'would quite'),\n",
       " (2.5657045367685942, 'would question'),\n",
       " (2.5657045367685942, 'would kick'),\n",
       " (2.5657045367685942, 'would im'),\n",
       " (2.5657045367685942, 'would exist'),\n",
       " (2.5657045367685942, 'would excuse'),\n",
       " (2.5657045367685942, 'would everything'),\n",
       " (2.5657045367685942, 'would encourage'),\n",
       " (2.5657045367685942, 'would earn'),\n",
       " (2.5657045367685942, 'would differ'),\n",
       " (2.5657045367685942, 'would deserve'),\n",
       " (2.5657045367685942, 'would definately'),\n",
       " (2.5657045367685942, 'would currently'),\n",
       " (2.5657045367685942, 'worthy buffet'),\n",
       " (2.5657045367685942, 'worthless they'),\n",
       " (2.5657045367685942, 'worth saying'),\n",
       " (2.5657045367685942, 'worth risking'),\n",
       " (2.5657045367685942, 'worth mentioning'),\n",
       " (2.5657045367685942, 'worst wireless'),\n",
       " (2.5657045367685942, 'worst tapas'),\n",
       " (2.5657045367685942, 'worst screen'),\n",
       " (2.5657045367685942, 'worst salmon'),\n",
       " (2.5657045367685942, 'worst qt'),\n",
       " (2.5657045367685942, 'worst pub'),\n",
       " (2.5657045367685942, 'worst plain'),\n",
       " (2.5657045367685942, 'worst pepperoni'),\n",
       " (2.5657045367685942, 'worst pei'),\n",
       " (2.5657045367685942, 'worst our'),\n",
       " (2.5657045367685942, 'worst ordered'),\n",
       " (2.5657045367685942, 'worst items'),\n",
       " (2.5657045367685942, 'worst greek'),\n",
       " (2.5657045367685942, 'worst golfing'),\n",
       " (2.5657045367685942, 'worst full'),\n",
       " (2.5657045367685942, 'worst fear'),\n",
       " (2.5657045367685942, 'worst expierence'),\n",
       " (2.5657045367685942, 'worst drive'),\n",
       " (2.5657045367685942, 'worst dilly'),\n",
       " (2.5657045367685942, 'worst cupcake'),\n",
       " (2.5657045367685942, 'worst chapel'),\n",
       " (2.5657045367685942, 'worst bday'),\n",
       " (2.5657045367685942, 'worst batch'),\n",
       " (2.5657045367685942, 'worse yoga'),\n",
       " (2.5657045367685942, 'worse retail'),\n",
       " (2.5657045367685942, 'worse kfc'),\n",
       " (2.5657045367685942, 'worse burgers'),\n",
       " (2.5657045367685942, 'worn u'),\n",
       " (2.5657045367685942, 'worn carpet'),\n",
       " (2.5657045367685942, 'world spaghetti'),\n",
       " (2.5657045367685942, 'world shuttered'),\n",
       " (2.5657045367685942, 'world right'),\n",
       " (2.5657045367685942, 'world only'),\n",
       " (2.5657045367685942, 'world offers'),\n",
       " (2.5657045367685942, 'world happened'),\n",
       " (2.5657045367685942, 'world enjoy'),\n",
       " (2.5657045367685942, 'world dining'),\n",
       " (2.5657045367685942, 'world championship'),\n",
       " (2.5657045367685942, 'works once'),\n",
       " (2.5657045367685942, 'workout without'),\n",
       " (2.5657045367685942, 'workout tanks'),\n",
       " (2.5657045367685942, 'workout program'),\n",
       " (2.5657045367685942, 'workout in'),\n",
       " (2.5657045367685942, 'workout equipment'),\n",
       " (2.5657045367685942, 'workout because'),\n",
       " (2.5657045367685942, 'working retail'),\n",
       " (2.5657045367685942, 'working outside'),\n",
       " (2.5657045367685942, 'working my'),\n",
       " (2.5657045367685942, 'working get'),\n",
       " (2.5657045367685942, 'working fridge'),\n",
       " (2.5657045367685942, 'working during'),\n",
       " (2.5657045367685942, 'workez does'),\n",
       " (2.5657045367685942, 'workez'),\n",
       " (2.5657045367685942, 'workers running'),\n",
       " (2.5657045367685942, 'workers demanded'),\n",
       " (2.5657045367685942, 'workers checked'),\n",
       " (2.5657045367685942, 'worker while'),\n",
       " (2.5657045367685942, 'worker type'),\n",
       " (2.5657045367685942, 'worker gave'),\n",
       " (2.5657045367685942, 'worked we'),\n",
       " (2.5657045367685942, 'worked themselves'),\n",
       " (2.5657045367685942, 'worked quickly'),\n",
       " (2.5657045367685942, 'worked great'),\n",
       " (2.5657045367685942, 'work who'),\n",
       " (2.5657045367685942, 'work tracey'),\n",
       " (2.5657045367685942, 'work salon'),\n",
       " (2.5657045367685942, 'work practices'),\n",
       " (2.5657045367685942, 'work normally'),\n",
       " (2.5657045367685942, 'work like'),\n",
       " (2.5657045367685942, 'work had'),\n",
       " (2.5657045367685942, 'work engine'),\n",
       " (2.5657045367685942, 'work drove'),\n",
       " (2.5657045367685942, 'work crew'),\n",
       " (2.5657045367685942, 'work buffet'),\n",
       " (2.5657045367685942, 'work bf'),\n",
       " (2.5657045367685942, 'work asked'),\n",
       " (2.5657045367685942, 'work amazing'),\n",
       " (2.5657045367685942, 'work 325'),\n",
       " (2.5657045367685942, 'wore constantly'),\n",
       " (2.5657045367685942, 'words shrimp'),\n",
       " (2.5657045367685942, 'word she'),\n",
       " (2.5657045367685942, 'word on'),\n",
       " (2.5657045367685942, 'word my'),\n",
       " (2.5657045367685942, 'word brasserie'),\n",
       " (2.5657045367685942, 'woot ers'),\n",
       " (2.5657045367685942, 'woods go'),\n",
       " (2.5657045367685942, 'wood but'),\n",
       " (2.5657045367685942, 'woo che'),\n",
       " (2.5657045367685942, 'wontons then'),\n",
       " (2.5657045367685942, 'wonton dumplings'),\n",
       " (2.5657045367685942, 'wont understand'),\n",
       " (2.5657045367685942, 'wont knock'),\n",
       " (2.5657045367685942, 'wondering many'),\n",
       " (2.5657045367685942, 'wonderfully fluffy'),\n",
       " (2.5657045367685942, 'wonderful trip'),\n",
       " (2.5657045367685942, 'wonderful torta'),\n",
       " (2.5657045367685942, 'wonderful saturday'),\n",
       " (2.5657045367685942, 'wonderful rye'),\n",
       " (2.5657045367685942, 'wonderful restaurants'),\n",
       " (2.5657045367685942, 'wonderful pittsburgh'),\n",
       " (2.5657045367685942, 'wonderful pasta'),\n",
       " (2.5657045367685942, 'wonderful only'),\n",
       " (2.5657045367685942, 'wonderful most'),\n",
       " (2.5657045367685942, 'wonderful meatball'),\n",
       " (2.5657045367685942, 'wonderful italian'),\n",
       " (2.5657045367685942, 'wonderful hubby'),\n",
       " (2.5657045367685942, 'wonderful however'),\n",
       " (2.5657045367685942, 'wonderful french'),\n",
       " (2.5657045367685942, 'wonderful flavors'),\n",
       " (2.5657045367685942, 'wonderful dance'),\n",
       " (2.5657045367685942, 'wonderful christian'),\n",
       " (2.5657045367685942, 'wonderful biking'),\n",
       " (2.5657045367685942, 'wonderful bartender1'),\n",
       " (2.5657045367685942, 'wonderful 2'),\n",
       " (2.5657045367685942, 'wondered the'),\n",
       " (2.5657045367685942, 'women staying'),\n",
       " (2.5657045367685942, 'women fitted'),\n",
       " (2.5657045367685942, 'woman she'),\n",
       " (2.5657045367685942, 'woman ran'),\n",
       " (2.5657045367685942, 'woman only'),\n",
       " (2.5657045367685942, 'woman moment'),\n",
       " (2.5657045367685942, 'woman asked'),\n",
       " (2.5657045367685942, 'wolff is'),\n",
       " (2.5657045367685942, 'wolff'),\n",
       " (2.5657045367685942, 'wolf lodge'),\n",
       " (2.5657045367685942, 'wok did'),\n",
       " (2.5657045367685942, 'wohl ein'),\n",
       " (2.5657045367685942, 'wohl'),\n",
       " (2.5657045367685942, 'woburn ma'),\n",
       " (2.5657045367685942, 'woburn'),\n",
       " (2.5657045367685942, 'wo fat'),\n",
       " (2.5657045367685942, 'wo etwas'),\n",
       " (2.5657045367685942, 'wizard haven'),\n",
       " (2.5657045367685942, 'wizard for'),\n",
       " (2.5657045367685942, 'witty they'),\n",
       " (2.5657045367685942, 'witnessed two'),\n",
       " (2.5657045367685942, 'without their'),\n",
       " (2.5657045367685942, 'without tasting'),\n",
       " (2.5657045367685942, 'without slamming'),\n",
       " (2.5657045367685942, 'without preordering'),\n",
       " (2.5657045367685942, 'without pilates'),\n",
       " (2.5657045367685942, 'without overpowering'),\n",
       " (2.5657045367685942, 'without drinking'),\n",
       " (2.5657045367685942, 'without checking'),\n",
       " (2.5657045367685942, 'without answering'),\n",
       " (2.5657045367685942, 'without alcohol'),\n",
       " (2.5657045367685942, 'within its'),\n",
       " (2.5657045367685942, 'within 3'),\n",
       " (2.5657045367685942, 'withdrawl on'),\n",
       " (2.5657045367685942, 'withdrawl'),\n",
       " (2.5657045367685942, 'with yogurt'),\n",
       " (2.5657045367685942, 'with wonton'),\n",
       " (2.5657045367685942, 'with wild'),\n",
       " (2.5657045367685942, 'with wfm'),\n",
       " (2.5657045367685942, 'with walls'),\n",
       " (2.5657045367685942, 'with vegas'),\n",
       " (2.5657045367685942, 'with unmistkable'),\n",
       " (2.5657045367685942, 'with typos'),\n",
       " (2.5657045367685942, 'with tunes'),\n",
       " (2.5657045367685942, 'with tint'),\n",
       " (2.5657045367685942, 'with thought'),\n",
       " (2.5657045367685942, 'with sunbrella'),\n",
       " (2.5657045367685942, 'with steven'),\n",
       " (2.5657045367685942, 'with stell'),\n",
       " (2.5657045367685942, 'with stairs'),\n",
       " (2.5657045367685942, 'with stained'),\n",
       " (2.5657045367685942, 'with sprouts'),\n",
       " (2.5657045367685942, 'with skills'),\n",
       " (2.5657045367685942, 'with showtime'),\n",
       " (2.5657045367685942, 'with shauna'),\n",
       " (2.5657045367685942, 'with sharp'),\n",
       " (2.5657045367685942, 'with seperate'),\n",
       " (2.5657045367685942, 'with self'),\n",
       " (2.5657045367685942, 'with seasoned'),\n",
       " (2.5657045367685942, 'with santa'),\n",
       " (2.5657045367685942, 'with rocks'),\n",
       " (2.5657045367685942, 'with ripped'),\n",
       " (2.5657045367685942, 'with rich'),\n",
       " (2.5657045367685942, 'with reds'),\n",
       " (2.5657045367685942, 'with quick'),\n",
       " (2.5657045367685942, 'with prices'),\n",
       " (2.5657045367685942, 'with ponzu'),\n",
       " (2.5657045367685942, 'with plumbing'),\n",
       " (2.5657045367685942, 'with phenomenal'),\n",
       " (2.5657045367685942, 'with phat'),\n",
       " (2.5657045367685942, 'with pedicure'),\n",
       " (2.5657045367685942, 'with peas'),\n",
       " (2.5657045367685942, 'with patrick'),\n",
       " (2.5657045367685942, 'with pastries'),\n",
       " (2.5657045367685942, 'with paige'),\n",
       " (2.5657045367685942, 'with overpowered'),\n",
       " (2.5657045367685942, 'with normal'),\n",
       " (2.5657045367685942, 'with natural'),\n",
       " (2.5657045367685942, 'with nathan'),\n",
       " (2.5657045367685942, 'with muffin'),\n",
       " (2.5657045367685942, 'with metal'),\n",
       " (2.5657045367685942, 'with mateo'),\n",
       " (2.5657045367685942, 'with massive'),\n",
       " (2.5657045367685942, 'with lynn'),\n",
       " (2.5657045367685942, 'with luxury'),\n",
       " (2.5657045367685942, 'with late'),\n",
       " (2.5657045367685942, 'with kimchee'),\n",
       " (2.5657045367685942, 'with jez'),\n",
       " (2.5657045367685942, 'with jersey'),\n",
       " (2.5657045367685942, 'with javiers'),\n",
       " (2.5657045367685942, 'with ipm'),\n",
       " (2.5657045367685942, 'with internet'),\n",
       " (2.5657045367685942, 'with infused'),\n",
       " (2.5657045367685942, 'with iceberg'),\n",
       " (2.5657045367685942, 'with hummous'),\n",
       " (2.5657045367685942, 'with hubby'),\n",
       " (2.5657045367685942, 'with hashbrowns'),\n",
       " (2.5657045367685942, 'with hardcore'),\n",
       " (2.5657045367685942, 'with guillain'),\n",
       " (2.5657045367685942, 'with greyhound'),\n",
       " (2.5657045367685942, 'with google'),\n",
       " (2.5657045367685942, 'with gooey'),\n",
       " (2.5657045367685942, 'with god'),\n",
       " (2.5657045367685942, 'with floor360'),\n",
       " (2.5657045367685942, 'with flight'),\n",
       " (2.5657045367685942, 'with fewer'),\n",
       " (2.5657045367685942, 'with fan'),\n",
       " (2.5657045367685942, 'with explanations'),\n",
       " (2.5657045367685942, 'with excellence'),\n",
       " (2.5657045367685942, 'with espresso'),\n",
       " (2.5657045367685942, 'with encore'),\n",
       " (2.5657045367685942, 'with eastover'),\n",
       " (2.5657045367685942, 'with dubious'),\n",
       " (2.5657045367685942, 'with drive'),\n",
       " (2.5657045367685942, 'with diverticulitis'),\n",
       " (2.5657045367685942, 'with dishonest'),\n",
       " (2.5657045367685942, 'with dining'),\n",
       " (2.5657045367685942, 'with designs'),\n",
       " (2.5657045367685942, 'with describing'),\n",
       " (2.5657045367685942, 'with dance'),\n",
       " (2.5657045367685942, 'with d'),\n",
       " (2.5657045367685942, 'with cleaners'),\n",
       " (2.5657045367685942, 'with classy'),\n",
       " (2.5657045367685942, 'with chriss'),\n",
       " (2.5657045367685942, 'with cerebral'),\n",
       " (2.5657045367685942, 'with caution'),\n",
       " (2.5657045367685942, 'with casual'),\n",
       " (2.5657045367685942, 'with carolina'),\n",
       " (2.5657045367685942, 'with caremore'),\n",
       " (2.5657045367685942, 'with cardboard'),\n",
       " (2.5657045367685942, 'with cacti'),\n",
       " (2.5657045367685942, 'with buttercream'),\n",
       " (2.5657045367685942, 'with bridgett'),\n",
       " (2.5657045367685942, 'with boston'),\n",
       " (2.5657045367685942, 'with blankets'),\n",
       " (2.5657045367685942, 'with barely'),\n",
       " (2.5657045367685942, 'with barbecue'),\n",
       " (2.5657045367685942, 'with banner'),\n",
       " (2.5657045367685942, 'with away'),\n",
       " (2.5657045367685942, 'with art'),\n",
       " (2.5657045367685942, 'with arriba'),\n",
       " (2.5657045367685942, 'with ariella'),\n",
       " (2.5657045367685942, 'with am'),\n",
       " (2.5657045367685942, 'with alterations'),\n",
       " (2.5657045367685942, 'with also'),\n",
       " (2.5657045367685942, 'with acrobatics'),\n",
       " (2.5657045367685942, 'wit cheesy'),\n",
       " (2.5657045367685942, 'wishes my'),\n",
       " (2.5657045367685942, 'wished the'),\n",
       " (2.5657045367685942, 'wish someone'),\n",
       " (2.5657045367685942, 'wish panda'),\n",
       " (2.5657045367685942, 'wiser choice'),\n",
       " (2.5657045367685942, 'wiser'),\n",
       " (2.5657045367685942, 'wisely but'),\n",
       " (2.5657045367685942, 'wise up'),\n",
       " (2.5657045367685942, 'wise like'),\n",
       " (2.5657045367685942, 'wisconsin for'),\n",
       " (2.5657045367685942, 'wires came'),\n",
       " (2.5657045367685942, 'wireless service'),\n",
       " (2.5657045367685942, 'wir wurden'),\n",
       " (2.5657045367685942, 'wir haben'),\n",
       " (2.5657045367685942, 'wir essen'),\n",
       " (2.5657045367685942, 'wiped it'),\n",
       " (2.5657045367685942, 'wipe away'),\n",
       " (2.5657045367685942, 'winter is'),\n",
       " (2.5657045367685942, 'winter even'),\n",
       " (2.5657045367685942, 'wins we'),\n",
       " (2.5657045367685942, 'winner try'),\n",
       " (2.5657045367685942, 'winner king'),\n",
       " (2.5657045367685942, 'wings you'),\n",
       " (2.5657045367685942, 'wings yet'),\n",
       " (2.5657045367685942, 'wings where'),\n",
       " (2.5657045367685942, 'wings soon'),\n",
       " (2.5657045367685942, 'wings since'),\n",
       " (2.5657045367685942, 'wings or'),\n",
       " (2.5657045367685942, 'wings here'),\n",
       " (2.5657045367685942, 'wings fresh'),\n",
       " (2.5657045367685942, 'wings due'),\n",
       " (2.5657045367685942, 'wings delivered'),\n",
       " (2.5657045367685942, 'wings cold'),\n",
       " (2.5657045367685942, 'wines a'),\n",
       " (2.5657045367685942, 'wines 3'),\n",
       " (2.5657045367685942, 'wine will'),\n",
       " (2.5657045367685942, 'wine which'),\n",
       " (2.5657045367685942, 'wine unfortunately'),\n",
       " (2.5657045367685942, 'wine suggestions'),\n",
       " (2.5657045367685942, 'wine seemed'),\n",
       " (2.5657045367685942, 'wine racks'),\n",
       " (2.5657045367685942, 'wine pub'),\n",
       " (2.5657045367685942, 'wine more'),\n",
       " (2.5657045367685942, 'wine lofts'),\n",
       " (2.5657045367685942, 'wine into'),\n",
       " (2.5657045367685942, 'wine go'),\n",
       " (2.5657045367685942, 'wine done'),\n",
       " (2.5657045367685942, 'wine cheese'),\n",
       " (2.5657045367685942, 'wine but'),\n",
       " (2.5657045367685942, 'wine bunk'),\n",
       " (2.5657045367685942, 'windy as'),\n",
       " (2.5657045367685942, 'windsor i'),\n",
       " (2.5657045367685942, 'windshields replaced'),\n",
       " (2.5657045367685942, 'windshields'),\n",
       " (2.5657045367685942, 'windshield chips'),\n",
       " (2.5657045367685942, 'windowsill at'),\n",
       " (2.5657045367685942, 'windowsill'),\n",
       " (2.5657045367685942, 'windows tinted'),\n",
       " (2.5657045367685942, 'windows overlooking'),\n",
       " (2.5657045367685942, 'windows a'),\n",
       " (2.5657045367685942, 'windows 7'),\n",
       " (2.5657045367685942, 'window tint'),\n",
       " (2.5657045367685942, 'window couldn'),\n",
       " (2.5657045367685942, 'window called'),\n",
       " (2.5657045367685942, 'winchels donuts'),\n",
       " (2.5657045367685942, 'winchels'),\n",
       " (2.5657045367685942, 'win in'),\n",
       " (2.5657045367685942, 'wimpy s'),\n",
       " (2.5657045367685942, 'wilted produce'),\n",
       " (2.5657045367685942, 'wilson parker'),\n",
       " (2.5657045367685942, 'willow was'),\n",
       " (2.5657045367685942, 'willingly sell'),\n",
       " (2.5657045367685942, 'william sonoma'),\n",
       " (2.5657045367685942, 'william j'),\n",
       " (2.5657045367685942, 'will won'),\n",
       " (2.5657045367685942, 'will wake'),\n",
       " (2.5657045367685942, 'will tear'),\n",
       " (2.5657045367685942, 'will summarize'),\n",
       " (2.5657045367685942, 'will smack'),\n",
       " (2.5657045367685942, 'will scam'),\n",
       " (2.5657045367685942, 'will ride'),\n",
       " (2.5657045367685942, 'will remain'),\n",
       " (2.5657045367685942, 'will redefine'),\n",
       " (2.5657045367685942, 'will reach'),\n",
       " (2.5657045367685942, 'will purchase'),\n",
       " (2.5657045367685942, 'will or'),\n",
       " (2.5657045367685942, 'will mandate'),\n",
       " (2.5657045367685942, 'will is'),\n",
       " (2.5657045367685942, 'will happily'),\n",
       " (2.5657045367685942, 'will happen'),\n",
       " (2.5657045367685942, 'will freeze'),\n",
       " (2.5657045367685942, 'will ensure'),\n",
       " (2.5657045367685942, 'will crave'),\n",
       " (2.5657045367685942, 'will bread'),\n",
       " (2.5657045367685942, 'will appreciate'),\n",
       " (2.5657045367685942, 'wildest indoor'),\n",
       " (2.5657045367685942, 'wild alaskan'),\n",
       " (2.5657045367685942, 'wilcox at'),\n",
       " (2.5657045367685942, 'wilcox'),\n",
       " (2.5657045367685942, 'wiht my'),\n",
       " (2.5657045367685942, 'wifi providing'),\n",
       " (2.5657045367685942, 'wifi my'),\n",
       " (2.5657045367685942, 'wifey had'),\n",
       " (2.5657045367685942, 'wifey got'),\n",
       " (2.5657045367685942, 'wife while'),\n",
       " (2.5657045367685942, 'wife since'),\n",
       " (2.5657045367685942, 'wife polish'),\n",
       " (2.5657045367685942, 'wife insisted'),\n",
       " (2.5657045367685942, 'wife attend'),\n",
       " (2.5657045367685942, 'widest variety'),\n",
       " (2.5657045367685942, 'widescreen hd'),\n",
       " (2.5657045367685942, 'wide noise'),\n",
       " (2.5657045367685942, 'wichcraft in'),\n",
       " (2.5657045367685942, 'why waitress'),\n",
       " (2.5657045367685942, 'why waiters'),\n",
       " (2.5657045367685942, 'why spring'),\n",
       " (2.5657045367685942, 'why spend'),\n",
       " (2.5657045367685942, 'why shawn'),\n",
       " (2.5657045367685942, 'why january'),\n",
       " (2.5657045367685942, 'why horrible'),\n",
       " (2.5657045367685942, 'why had'),\n",
       " (2.5657045367685942, 'why an'),\n",
       " (2.5657045367685942, 'whose orders'),\n",
       " (2.5657045367685942, 'whose idea'),\n",
       " (2.5657045367685942, 'whose charm'),\n",
       " (2.5657045367685942, 'whom throughout'),\n",
       " (2.5657045367685942, 'whom administered'),\n",
       " (2.5657045367685942, 'wholesale club'),\n",
       " (2.5657045367685942, 'whole week'),\n",
       " (2.5657045367685942, 'whole variety'),\n",
       " (2.5657045367685942, 'whole vacation'),\n",
       " (2.5657045367685942, 'whole spareribs'),\n",
       " (2.5657045367685942, 'whole selection'),\n",
       " (2.5657045367685942, 'whole paycheck'),\n",
       " (2.5657045367685942, 'whole nine'),\n",
       " (2.5657045367685942, 'whole heartedly'),\n",
       " (2.5657045367685942, 'whole decor'),\n",
       " (2.5657045367685942, 'who walks'),\n",
       " (2.5657045367685942, 'who views'),\n",
       " (2.5657045367685942, 'who stated'),\n",
       " (2.5657045367685942, 'who sings'),\n",
       " (2.5657045367685942, 'who showed'),\n",
       " (2.5657045367685942, 'who selected'),\n",
       " (2.5657045367685942, 'who sadly'),\n",
       " (2.5657045367685942, 'who posts'),\n",
       " (2.5657045367685942, 'who love'),\n",
       " (2.5657045367685942, 'who loke'),\n",
       " (2.5657045367685942, 'who interact'),\n",
       " (2.5657045367685942, 'who himself'),\n",
       " (2.5657045367685942, 'who engaged'),\n",
       " (2.5657045367685942, 'who deliver'),\n",
       " (2.5657045367685942, 'who deleted'),\n",
       " (2.5657045367685942, 'who by'),\n",
       " (2.5657045367685942, 'who basically'),\n",
       " (2.5657045367685942, 'whitey s'),\n",
       " (2.5657045367685942, 'whitey'),\n",
       " (2.5657045367685942, 'whites or'),\n",
       " (2.5657045367685942, 'white swirly'),\n",
       " (2.5657045367685942, 'white sugar'),\n",
       " (2.5657045367685942, 'white spinach'),\n",
       " (2.5657045367685942, 'white shake'),\n",
       " (2.5657045367685942, 'white particles'),\n",
       " (2.5657045367685942, 'white mold'),\n",
       " (2.5657045367685942, 'white lower'),\n",
       " (2.5657045367685942, 'white handkerchiefs'),\n",
       " (2.5657045367685942, 'white hair'),\n",
       " (2.5657045367685942, 'white bobbed'),\n",
       " (2.5657045367685942, 'white again'),\n",
       " (2.5657045367685942, 'whispering while'),\n",
       " (2.5657045367685942, 'whispering'),\n",
       " (2.5657045367685942, 'whisky professionals'),\n",
       " (2.5657045367685942, 'whirpoolgold 28'),\n",
       " (2.5657045367685942, 'whirpoolgold'),\n",
       " (2.5657045367685942, 'whim when'),\n",
       " (2.5657045367685942, 'whim absolutely'),\n",
       " (2.5657045367685942, 'whilst not'),\n",
       " (2.5657045367685942, 'while touring'),\n",
       " (2.5657045367685942, 'while suffering'),\n",
       " (2.5657045367685942, 'while recently'),\n",
       " (2.5657045367685942, 'while hilton'),\n",
       " (2.5657045367685942, 'while helping'),\n",
       " (2.5657045367685942, 'while due'),\n",
       " (2.5657045367685942, 'while drinking'),\n",
       " (2.5657045367685942, 'while disrupting'),\n",
       " (2.5657045367685942, 'while casino'),\n",
       " (2.5657045367685942, 'while asking'),\n",
       " (2.5657045367685942, 'while 3rd'),\n",
       " (2.5657045367685942, 'which takes'),\n",
       " (2.5657045367685942, 'which sticks'),\n",
       " (2.5657045367685942, 'which speeds'),\n",
       " (2.5657045367685942, 'which several'),\n",
       " (2.5657045367685942, 'which sent'),\n",
       " (2.5657045367685942, 'which room'),\n",
       " (2.5657045367685942, 'which risque'),\n",
       " (2.5657045367685942, 'which restaurants'),\n",
       " (2.5657045367685942, 'which playing'),\n",
       " (2.5657045367685942, 'which others'),\n",
       " (2.5657045367685942, 'which not'),\n",
       " (2.5657045367685942, 'which left'),\n",
       " (2.5657045367685942, 'which essential'),\n",
       " (2.5657045367685942, 'which entitled'),\n",
       " (2.5657045367685942, 'which beats'),\n",
       " (2.5657045367685942, 'which arrived'),\n",
       " (2.5657045367685942, 'whew what'),\n",
       " (2.5657045367685942, 'where uptown'),\n",
       " (2.5657045367685942, 'where underneath'),\n",
       " (2.5657045367685942, 'where something'),\n",
       " (2.5657045367685942, 'where secret'),\n",
       " (2.5657045367685942, 'where right'),\n",
       " (2.5657045367685942, 'where on'),\n",
       " (2.5657045367685942, 'where men'),\n",
       " (2.5657045367685942, 'where megabus'),\n",
       " (2.5657045367685942, 'where hipsters'),\n",
       " (2.5657045367685942, 'where close'),\n",
       " (2.5657045367685942, 'where charlene'),\n",
       " (2.5657045367685942, 'where bouchon'),\n",
       " (2.5657045367685942, 'whenever u'),\n",
       " (2.5657045367685942, 'whendy the'),\n",
       " (2.5657045367685942, 'whendy'),\n",
       " (2.5657045367685942, 'when trends'),\n",
       " (2.5657045367685942, 'when thunder'),\n",
       " (2.5657045367685942, 'when thirty'),\n",
       " (2.5657045367685942, 'when taken'),\n",
       " (2.5657045367685942, 'when stuff'),\n",
       " (2.5657045367685942, 'when school'),\n",
       " (2.5657045367685942, 'when other'),\n",
       " (2.5657045367685942, 'when making'),\n",
       " (2.5657045367685942, 'when just'),\n",
       " (2.5657045367685942, 'when fedex'),\n",
       " (2.5657045367685942, 'when claiming'),\n",
       " (2.5657045367685942, 'when at'),\n",
       " (2.5657045367685942, 'wheelhouse lounge'),\n",
       " (2.5657045367685942, 'wheelhouse'),\n",
       " (2.5657045367685942, 'wheat salmon'),\n",
       " (2.5657045367685942, 'whatsoever now'),\n",
       " (2.5657045367685942, 'whatever vehicle'),\n",
       " (2.5657045367685942, 'whatever service'),\n",
       " (2.5657045367685942, 'whatever next'),\n",
       " (2.5657045367685942, 'whatever lines'),\n",
       " (2.5657045367685942, 'what yoga'),\n",
       " (2.5657045367685942, 'what yelpers'),\n",
       " (2.5657045367685942, 'what these'),\n",
       " (2.5657045367685942, 'what struck'),\n",
       " (2.5657045367685942, 'what side'),\n",
       " (2.5657045367685942, 'what samantha'),\n",
       " (2.5657045367685942, 'what patrick'),\n",
       " (2.5657045367685942, 'what optimistic'),\n",
       " (2.5657045367685942, 'what led'),\n",
       " (2.5657045367685942, 'what last'),\n",
       " (2.5657045367685942, 'what italian'),\n",
       " (2.5657045367685942, 'what enticed'),\n",
       " (2.5657045367685942, 'what crossfit'),\n",
       " (2.5657045367685942, 'what city'),\n",
       " (2.5657045367685942, 'what charles'),\n",
       " (2.5657045367685942, 'what beautiful'),\n",
       " (2.5657045367685942, 'wfm esp'),\n",
       " (2.5657045367685942, 'wet walnuts'),\n",
       " (2.5657045367685942, 'westgate las'),\n",
       " (2.5657045367685942, 'western kind'),\n",
       " (2.5657045367685942, 'western barn'),\n",
       " (2.5657045367685942, 'westbrook animal'),\n",
       " (2.5657045367685942, 'westbrook'),\n",
       " (2.5657045367685942, 'west guns'),\n",
       " (2.5657045367685942, 'west end'),\n",
       " (2.5657045367685942, 'west corner'),\n",
       " (2.5657045367685942, 'west and'),\n",
       " (2.5657045367685942, 'wesley and'),\n",
       " (2.5657045367685942, 'werent serving'),\n",
       " (2.5657045367685942, 'werent honoring'),\n",
       " (2.5657045367685942, 'were written'),\n",
       " (2.5657045367685942, 'were whatever'),\n",
       " (2.5657045367685942, 'were went'),\n",
       " (2.5657045367685942, 'were weary'),\n",
       " (2.5657045367685942, 'were wack'),\n",
       " (2.5657045367685942, 'were until'),\n",
       " (2.5657045367685942, 'were undecided'),\n",
       " (2.5657045367685942, 'were tolerable'),\n",
       " (2.5657045367685942, 'were swimming'),\n",
       " (2.5657045367685942, 'were strolling'),\n",
       " (2.5657045367685942, 'were showing'),\n",
       " (2.5657045367685942, 'were samples'),\n",
       " (2.5657045367685942, 'were ridged'),\n",
       " (2.5657045367685942, 'were ribs'),\n",
       " (2.5657045367685942, 'were redecorating'),\n",
       " (2.5657045367685942, 'were pure'),\n",
       " (2.5657045367685942, 'were poached'),\n",
       " (2.5657045367685942, 'were mind'),\n",
       " (2.5657045367685942, 'were meh'),\n",
       " (2.5657045367685942, 'were kept'),\n",
       " (2.5657045367685942, 'were interested'),\n",
       " (2.5657045367685942, 'were inconsistent'),\n",
       " (2.5657045367685942, 'were holiday'),\n",
       " (2.5657045367685942, 'were herded'),\n",
       " (2.5657045367685942, 'were heavy'),\n",
       " (2.5657045367685942, 'were harmed'),\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(differens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### most_confident_prediction( fix cutoff) + dev stop increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 1 : dev accuracy: 0.8078602620087336\n",
      "         train size:   4582\n",
      "loop 2 : dev accuracy: 0.8078602620087336\n",
      "         train size:   19602\n",
      "loop 3 : dev accuracy: 0.8013100436681223\n",
      "         train size:   34557\n",
      "loop 4 : dev accuracy: 0.7882096069868996\n",
      "         train size:   45763\n",
      "loop 5 : dev accuracy: 0.7903930131004366\n",
      "         train size:   52570\n",
      "loop 6 : dev accuracy: 0.7838427947598253\n",
      "         train size:   56310\n",
      "loop 7 : dev accuracy: 0.7838427947598253\n",
      "         train size:   58369\n",
      "loop 8 : dev accuracy: 0.7751091703056768\n",
      "         train size:   59611\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "cur_train_data = deepcopy(sentiment.train_data)\n",
    "cur_trainy = deepcopy(sentiment.trainy)\n",
    " \n",
    "unlabel_data = deepcopy(unlabeled.data)\n",
    "testX = unlabeled.X\n",
    "pre_acc = 0\n",
    "wordsId, idf_table_1 = tokenize(cur_train_data)\n",
    "index = 0\n",
    "while True:\n",
    "    index += 1\n",
    "    \n",
    "    wordsId, idf_table = tokenize(cur_train_data)\n",
    "    trainX = tfidf_matrix(cur_train_data, wordsId, idf_table)\n",
    "    devX = tfidf_matrix(sentiment.dev_data, wordsId, idf_table)\n",
    "    cls = train_classifier(trainX, cur_trainy, 0.1)\n",
    "    \n",
    "    dev_yp = cls.predict(devX)\n",
    "    dev_acc = metrics.accuracy_score(sentiment.devy, dev_yp)\n",
    "    \n",
    "    print(\"loop\", index, \": dev accuracy:\", dev_acc)\n",
    "    print(\"         train size:  \", len(cur_train_data))\n",
    "    if index == 8:\n",
    "        break\n",
    "    pre_acc = dev_acc\n",
    "        \n",
    "    testX = tfidf_matrix(unlabel_data, wordsId, idf_table)\n",
    "    test_yp = cls.predict(testX)\n",
    "    test_confidence = abs(cls.decision_function(testX))\n",
    "    \n",
    "    expand_data = []\n",
    "    expand_y = []    \n",
    "    \n",
    "    for i in range(len(test_confidence)):\n",
    "        if test_confidence[i] > 3.5:\n",
    "            expand_data.append(unlabeled.data[i])\n",
    "            expand_y.append(test_yp[i])\n",
    "    \n",
    "    cur_train_data = sentiment.train_data + expand_data\n",
    "    cur_trainy = np.concatenate((sentiment.trainy, np.array(expand_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_yp = cls.predict(devX)\n",
    "error_list = []\n",
    "for i in range(len(sentiment.dev_data)):\n",
    "    if dev_yp[i] != sentiment.devy[i]:\n",
    "        error_list.append((sentiment.dev_data[i], dev_yp[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seeeeeeeeeeehr wunderbares Geschaeft fuer Menschen mit der Liebe zum Espresso.Herr Koeberl verkauft nicht nur Espressomaschinen, nein, man kann auch einen herausragend leckeren Espresso (oder auch 2) bei ihm trinken.Er fuehrt',\n",
       "  0),\n",
       " ('If you want your fancy flavored iced coffee, sure go here. But if you want a good version of their namesake- doughnuts, then go elsewhere. Specifically, go south to Fresh',\n",
       "  1),\n",
       " ('We decided to try hash house a go go after reading several positive yelp reviews.We were a little put off when we got to the Imperial Palace location at approximately',\n",
       "  0),\n",
       " ('Had an awsome late lunch at this bbq restaurant this afternoon.  I have a gluten alergy so it was a relief to be able to talk to the owner',\n",
       "  0),\n",
       " (\"You are guaranteed a headache because of all of the annoying, loud advertisements in your face while you're filling up your car. It's out of control.\",\n",
       "  1),\n",
       " (\"This location is fairly large compared to other's I've been to.  I prefer this location over the one in Chandler.  There is a lot more to choose from\",\n",
       "  0),\n",
       " ('The atmosphere, granted I say is pretty nice to be sipping on  your glass of white wine under some realistic clouds as your overheard and looking at people pass',\n",
       "  1),\n",
       " (\"Cons: Not-so-friendly service and cash only. You don't spend much time with them anyways so it's not a big deal. Slightly expensive. The portion sizes are medium sized, so not\",\n",
       "  0),\n",
       " (\"Cheese steak = 5 starsService = 2 starsWaiting time = 0 starsWe really want to love this place, but they're making it very hard for us to do so. We\",\n",
       "  1),\n",
       " (\"There is always a line running all the way out the door for dinner time.  Sometimes you can wait for an hour or less but sometimes it's more.\",\n",
       "  1),\n",
       " ('My wife and I had dinner here around 4:30 pm. We were seated right away and the waiter started the drinks immediately! My wife ordered the honey chicken with brown',\n",
       "  0),\n",
       " ('My Husband, Daughter, and I went to this place to eat a couple nights ago. You could tell the people who work there are nice, but the service was just',\n",
       "  0),\n",
       " (\"We found the place by Zen navigation so I expected it to be perfect. My cohort wasn't as stoked but still leaning toward a great night. We nabbed an older\",\n",
       "  0),\n",
       " ('guy with the turban-----not friendly!', 1),\n",
       " (\"Been here a handful of times, I usually bring my church toga, which is a very important piece of thread to me. So if anything happened to it, I'd be\",\n",
       "  0),\n",
       " (\"This place has decent crepe. I got the blueberry and banana crepe. It's a healthier alternative to the strawberry Nutella one I wanted to get when I walked by this\",\n",
       "  0),\n",
       " ('Please don\\'t confuse \"Mekong Plaza\" with \"Mekong Palace\".\"Mekong Plaza\" is one of the finest Oriental shopping center In Mesa (my opinion).\"Mekong Palace\" is a newly open Chinese Dim Sum diner',\n",
       "  1),\n",
       " (\"OK, to be fair, my fellow Yelpers must know that the Manager of Tom's Thumb has attempted to remedy my last review by asking me to come again.  I\",\n",
       "  0),\n",
       " ('Smaller portion than it in the town square. More rooming but long wait to get the food',\n",
       "  0),\n",
       " ('Although I live within walking distance, I always passed on Tailgaters. Not sure I had a good reason, other than I was used to going to other local bars to',\n",
       "  0),\n",
       " ('They finally opened a DD in Cali but I refuse to drive to Santa Monica and wait in a long line to try the coffee. Was excited to get a',\n",
       "  0),\n",
       " ('Dear Allen -- I advise you NOT to come back for dinner.We are big, BIG Tommy Bahama fans.  We have a house on Maui with lots of Tommy barware.',\n",
       "  1),\n",
       " ('My dry cleaner of choice in Charlotte. Other than closing a bit early for my taste- they do open bright and early. Sign up for their mailing list. They send',\n",
       "  0),\n",
       " (\"I'll be straight up on here.  I am definitely a club rat, I love music, dancing, and hanging out w/ friends, to have a good time, etc.  The\",\n",
       "  1),\n",
       " (\"Or Blackout Centrale, as I like to refer to it.  This place is notorious in my book for the fact that every time I've ever gone, I have lost\",\n",
       "  0),\n",
       " (\"$20 foot massage for an hour. Not entirely only working on the foot. They syart with the head, shoulders, arms, foot, legs and then the back...for $20! Couldn't really understand\",\n",
       "  0),\n",
       " ('Wow, this is a hard one. I have been here several times for work functions and this last time for a nice Date Night with my husband. Hands down one',\n",
       "  1),\n",
       " (\"NOT very kid friendly candy shop. Very cute little store especially just steps away from the children's play area at the container park. BUT just as Jeff R. mentioned in\",\n",
       "  1),\n",
       " ('Benelux on Wellington frustrated me beyond words last night.After finishing a Zumba class across the street, a friend and I popped in here for  a quick drink and something',\n",
       "  1),\n",
       " ('Best thing about Chin Chin - SERVICE BABY!  These people know the true meaning of it!  When you pass by the restaurant while walking through NYNY you will',\n",
       "  0),\n",
       " ('I usually go to the Lennys on 35th and thunderbird, but this one was closer to my house. It was my first time going to this lennys and i must',\n",
       "  1),\n",
       " (\"We stopped here for dinner before we saw a movie. It's sort of almost like a pasta version of Chipotle, although building your own bowl of pasta would get pricey\",\n",
       "  0),\n",
       " ('Can I possibly give 10 stars?!?!!!? Anyway I should have written this review 2 days ago but I was too busy loving my hair...my apologies. I contacted Katy though email',\n",
       "  0),\n",
       " ('Mark came out today and cleaned our carpets very well. He is extremely professional, arrived a few minutes early, and gave us an honest estimate up front with all treatments',\n",
       "  0),\n",
       " ('We had our 2 dogs groomed   our little dog definitely needed her hair cut, but our older dog was just going to get a comb out for shedding.',\n",
       "  0),\n",
       " ('These cupcakes sure are pretty on the outside but they do not taste as good as they look. A coworker purchased these for an office party and everyone was very',\n",
       "  1),\n",
       " ('Wow not impressed wit the mens clothing prices. There a whole bunch of higher end store with this price range. The store crew was amazing and helpful! Really nice people.',\n",
       "  1),\n",
       " ('You guys blew it Tuscany Hotel & Casino.  This past weekend we stayed for 4 nights ((a long weekend and to celebrate our wedding anniversary) and made the reservations',\n",
       "  1),\n",
       " (\"The gluten free waffle is really great and it is nice to see gluten free on the menu, but the last two times I've gone they have been out of\",\n",
       "  1),\n",
       " (\"I came here last Saturday night with my family and I'm not sure what we were thinking but we didn't try to make a reservation until 2:00pm. That left us\",\n",
       "  0),\n",
       " (\"I've been to Macayo's....meh.  I like Ajo Al's better but service was a little spotty the last time I was there.  I have been looking for a go\",\n",
       "  0),\n",
       " (\"The food is subpar here. The Rio is way better.I came here because it's part of the Buffet of Buffets.Go for the Cotton candy and pre-sliced crab legs. The chinese\",\n",
       "  1),\n",
       " (\"I've been looking forward to trying Khotan ever since social house closed and I finally got to check it out.This review isn't focused on the food as much as it\",\n",
       "  0),\n",
       " ('I have heard the old saying  \"If you\\'re willing to spend a few bucks in Vegas, you will never get a bad meal\".   Well, another  Vegas',\n",
       "  1),\n",
       " ('Summary:  Beautiful hotel, highly error-prone staff. It is rare to find a hotel as beautiful as this. The look is very modern which I happen to love. It has',\n",
       "  1),\n",
       " ('If a casino or hotel has one thing then everyone else has to have one too.  There are few better examples of this than burger restaurants, and some are',\n",
       "  0),\n",
       " (\"I went in for an umbrella for our umbrella stroller but they didn't have it.  However, I had three of the nicest employees helping me out which I greatly\",\n",
       "  0),\n",
       " ('Beware...   hold  your  wallet  as  tight  as  you  can,  unless  you  belong  to  those  who',\n",
       "  1),\n",
       " (\"So, I'm walking out to my car after a long day at work. I reach into my purse to grab my phone and SMASH! My link to the rest of\",\n",
       "  0),\n",
       " (\"À éviter à tout prix. Leur hot-dog à 79 sous n'en vaut pas le détour, c'était mauvais autant au goût que pour la digestion. Leur poutine est probablement la pire\",\n",
       "  1),\n",
       " ('After being gone from the Las Vegas scene for a few years I was amazed at how many new celebrity owned restaurants had popped up. My boyfriend and I both',\n",
       "  1),\n",
       " ('A fabulous job done. My car was sparkling clean, no residue of my highway travels from California noticed anywhere on my vehicle.  *I got the Basic* Plus it was',\n",
       "  0),\n",
       " (\"I'm not from NY and don't claim that this is authentic NY pizza as I wouldn't know, but this is delicious.  What more can I say.  For me\",\n",
       "  0),\n",
       " (\"AJ is amazing fixed my iphone 5 in minutes I wasn't recieving calls or texts at all needed new SIM card and I went to a different store they couldn't\",\n",
       "  0),\n",
       " ('Absolutely Fantabulous!!!!! The surrounding atmosphere, the place settings, the SERVICE! The quality of the food------Exceptional for the location and the ambiance. There was not one negative thing I can write',\n",
       "  0),\n",
       " (\"They've opened a new coffee shop in Verrado and a 10 min stop for coffee has turned into a long laborious affair.  If you want to be in a\",\n",
       "  1),\n",
       " (\"I am Gluten intolerant, so eating out is very limiting. I was so pleased to see a note on Cracked Egg's dry erase board that said to ask about their\",\n",
       "  0),\n",
       " (\"I've been here off and on over the past few years and didn't make a visit this year until today! I enjoyed eating here when I used to work at\",\n",
       "  0),\n",
       " ('Its an average South Side bar.  They have a good beer selection on draft and in bottles.  The prices for drinks are pretty average for South Side as',\n",
       "  1),\n",
       " ('I came to The Olive Branch Bistro in Bruntsfield on Saturday at about 6pm with my mum.  From the menu outside it looked pretty expensive with with some mains',\n",
       "  0),\n",
       " (\"I'd been to the chain pet stores getting fish for my son.  I got bad information or no information and ended up killing fish because of the bad information.\",\n",
       "  0),\n",
       " (\"Ok so this is not fast pizza so if you need it in ten minutes and don't really care what it tastes like then this may not be your place.\",\n",
       "  0),\n",
       " ('First off, good service. Waitress was very friendly and honest. Anyways, I wanted some ramen this evening. Strolled in to find that today there was some ramen with chicken in',\n",
       "  1),\n",
       " ('Worth the extra couple dollars for tickets. Te seats recline and are super comfortable, be careful not to fall asleep!',\n",
       "  0),\n",
       " ('Gordon Ramsay, I love you but only four stars for this one & not because of the food. The hostesses are not very kind and the waiter (wish I remembered',\n",
       "  0),\n",
       " ('This review is regarding the service of TJ, one of the sales associates. My fiancé and I walked into CarMax yesterday evening with the intention of checking out a few',\n",
       "  0),\n",
       " ('THE GOOD: Its downtown.  If you havent been downtown in a while, its time to go.  This stay started out great, but lost some steam each day, and',\n",
       "  1),\n",
       " ('Two words - deliciously expensive.So our group walked in at 12:30 AM on Sunday without booking and we had to wait about 15 minutes to get seated. There was a',\n",
       "  0),\n",
       " ('Oh goodness, I was the envy of the table when I ordered the bbq chicken pizza.  When it came to the table it was on a wooden paddle and',\n",
       "  0),\n",
       " ('Normally LOVE Penn Station, but today was. Greeted by a gnat infestation @ my Penn Station. Sooo troubled by that! Please get rid of the gnats. I will be back',\n",
       "  1),\n",
       " ('Easy phone order and pick up of shrimp fried rice. This second order tasted better than my first Sunday evening order. Generous portions and fresh! A tad of soy sauce',\n",
       "  0),\n",
       " (\"There wasn't much to pick from we were looking to get a cake made ...prices a little high and it was hard to try to get exactly what we were\",\n",
       "  0),\n",
       " ('Let me just say that I am not a fan of buffets in general, but this place made a believer of a GOOD buffet.  If you love seafood, this',\n",
       "  0),\n",
       " (\"I had been to this place more than a few times and tried the chicken in just about every shape and form. I've had the Chicken Tacos, the Chicken Burrito,\",\n",
       "  1),\n",
       " (\"What's there to not like about this place. I was looking for a Yogurtland and instead went in here. The name doesn't fit the interior at all. I, for some\",\n",
       "  0),\n",
       " ('After doing some research, I found Jeremy on Yelp. Took my laptop to his place, he fixed our problem within a couple hours and we got our laptop back the',\n",
       "  0),\n",
       " ('My wife and I were looking to redo all of the floors in our house before our baby arrived, and thankfully we found this company! We had gotten off the',\n",
       "  0),\n",
       " ('We are Gilbert residents and love what is happening with the downtown area. Every time a new restaurant opens, we jump at the chance to try it. None have warranted',\n",
       "  1),\n",
       " (\"I get lazy and like home delivery. I don't know of any other restaurant in my area where I can get full meals plus pie delivered. We have patronized My\",\n",
       "  0),\n",
       " ('You walk in and are almost immedietly blinded by the Pink Neon.....  after a few minutes your eyes adjust to the brightness and you can focus on the Kitchy',\n",
       "  1),\n",
       " (\"It's customary to wait 45 minutes for a table here on the weekends. The unusual part is that I wait joyously and eager for a table the entire wait. I\",\n",
       "  0),\n",
       " (\"Really, Pittsburgh? The Primanti bros cheeseteak is the best sandwich in the world? Didn't make me very happy. The freezer-burned soggy fries, bland bread, and gross hamburger patty just didn't\",\n",
       "  1),\n",
       " (\"I didn't eat here, so I can't speak to the food, but the service was great, and they had a TON of beers.  Price was right in line with\",\n",
       "  0),\n",
       " ('Great show.  They were passing out $20 coupons.  I was confused about the seating, but my advice is to either get the cheap seats with the coupon, which',\n",
       "  0),\n",
       " ('We ordered the Dan Dan noodles and Thai coconut curry chicken. The noodles were a little over cooked but still very tasty. The curry was bland, peppers were cold, and',\n",
       "  1),\n",
       " (\"This restaurant is always empty and now I understand why.... It's byow and the food is really good from the tartare, to the bavette and l'agneau. But for the service\",\n",
       "  1),\n",
       " (\"I read all of the great reviews so I decided to pop in this afternoon, fire up the ole' laptop and get some work done. French Press sealed the deal\",\n",
       "  0),\n",
       " ('Of all of the hobby/craft stores you can go to in the Champaign area, I am not a fan of Jo-Ann Fabrics & Crafts. The store is too small: their',\n",
       "  1),\n",
       " (\"From best to worst. This restaurant in very few months from being my favorite became really bad. Here's a message to mister DB: if you were sitting at my table\",\n",
       "  1),\n",
       " ('This place is not good. Unfortunately it is one of the only all ages venues for bands to play at in the metro area, so good bands get stuck here.The',\n",
       "  1),\n",
       " ('Kabuki never disappoints. I wish I still lived in AZ so I could go back but then again that could be a bad thing.Their Lotus on Fire Roll is my',\n",
       "  0),\n",
       " ('Quite often pop in here if in Stockbridge.Nice addition to Stockbridge, not had food here yet but been for drinks a few times and they have spaces for locking up',\n",
       "  0),\n",
       " ('Having trouble on what to have for dinner and have a big appetite, Check out this place The desserts are great and so is the fresh food. I love coming',\n",
       "  1),\n",
       " ('My friends and I have been to Vegas our fair share, and the last time we went, we decided to stay at Aria.  I was very impressed - the',\n",
       "  0),\n",
       " ('My husband and I are new to Phoenix and have decided to try as many local places as possible. Intrigued by the outside decor, we ventured into this odd place.',\n",
       "  1),\n",
       " (\"Same great pizza, different location!Business seems to be picking up at the new location, with its proximity to Trader Joe's and Turnstyle. We've been here a few times, both for\",\n",
       "  0),\n",
       " (\"It's been a while since I ate at this cafe, but the experience was so good that it stays with me. My friend and I were traveling through Arizona on\",\n",
       "  0),\n",
       " ('First, the good news. The food was tasty. Every other part of the meal was a disaster. My burger was missing everything but meat and cheese. The cheese fries had',\n",
       "  1),\n",
       " ('Vor einigen Wochen habe ich in diesem Restaurant in Ermangelung einer Qype Empfehlung in der Umgebung (die geöffnet hatte) hier zu Abend gegessen. Ich muss ehrlich sagen: Ich war maßlos',\n",
       "  1),\n",
       " (\"Don't ask my why, but I was pleasantly surprised and not expecting to find a place like the Living Room hidden on Queen Creek Road. But, I'm happy to see\",\n",
       "  0),\n",
       " ('Finally! Saturday My friend and I went to get our Haircuts because I bragged about how amazing they are! My friend ended up getting there early in which Kim then',\n",
       "  0),\n",
       " (\"Dropping a 5 start on Charm Thai. It's got a slightly fancier attitude than your general Thai place, but for good reason. They turn out some delicious food. I haven't\",\n",
       "  0),\n",
       " ('Wtpho?! Exactly what I thought when we arrived in the parking lot.The face front not to much to say about it just looks like a normal business. Inside was nice',\n",
       "  0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictions to a file\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing predictions to a file\")\n",
    "unlabeled.X = tfidf_matrix(unlabeled.data, wordsId, idf_table)\n",
    "write_pred_kaggle_file(unlabeled, cls, \"data/sentiment-pred.csv\", sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### not using unlabeled at first\n",
    "k = 6: 0.8013 -> 0.7948\n",
    "       4582 -> 10401\n",
    "k = 5.75: 0.8013 -> 0.7948\n",
    "          4582 -> 11572\n",
    "k = 5.5: 0.8013 -> 0.8034 -> 0.8100 -> 0.7991\n",
    "         3582 -> 12868 -> 17310 -> 21104\n",
    "k = 5.25: 0.8013 -> 0.8056 -> 0.8079 -> 0.8057\n",
    "          4582 -> 14412 -> 20496 -> 25721\n",
    "k = 5: 0.8013 -> 0.8057 -> 0.8035\n",
    "       4582 -> 16109 -> 24104\n",
    "k = 4.5: 0.8013 -> 0.8035 -> 0.8035\n",
    "         4582 -> 20219 -> 32489\n",
    "k = 4: 0.8013 -> 0.8035 -> 0.8057 -> 0.7926\n",
    "       4582 -> 25301 -> 41856 -> 52429\n",
    "k = 3: 0.8013 -> 0.8035 -> 0.7991\n",
    "       4582 -> 38345 -> 60895"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding using word2vec and tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_table(dataset):\n",
    "    documentCount = defaultdict(int)\n",
    "    for document in dataset:\n",
    "        for w in set(document):\n",
    "            documentCount[w] += 1\n",
    "    idf = defaultdict(float)\n",
    "    for w in documentCount:\n",
    "        idf[w] = math.log(len(dataset) / documentCount[w])\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_table(document):\n",
    "    tf = defaultdict(int)\n",
    "    for w in document:\n",
    "        tf[w] += 1    \n",
    "    for w in tf:\n",
    "        tf[w] = math.log(1 + tf[w])\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_feature_matrix(dataset, idf, wv2):\n",
    "    matrix = []\n",
    "    for document in dataset:\n",
    "        feature = np.zeros(length)\n",
    "        base = 0\n",
    "        tf = tf_table(document)\n",
    "        for w in document:\n",
    "            if w in wv2:\n",
    "                \"\"\"\n",
    "                feature += wv2[w] * tf[w] * idf[w]\n",
    "                base += tf[w] * idf[w]\n",
    "                \"\"\"\n",
    "                feature += wv2[w]\n",
    "                base += 1\n",
    "                \n",
    "                \n",
    "        if base == 0:\n",
    "            matrix.append(feature)\n",
    "        else:\n",
    "            matrix.append(feature / base)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(raw_data):\n",
    "    tokenized_data = []\n",
    "    for d in raw_data:\n",
    "        lower = ''.join([c for c in d.lower()])\n",
    "        r = re.split(r'\\W+', lower)\n",
    "        r = [w for w in r]\n",
    "        tokenized_data.append(r)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "train_data = tokenize(sentiment.train_data)\n",
    "dev_data = tokenize(sentiment.dev_data)\n",
    "test_data = tokenize(unlabeled.data)\n",
    "length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_data + test_data \n",
    "model = gensim.models.Word2Vec(corpus, size=length, window=8, min_count=1, workers=10)\n",
    "model.train(corpus, total_examples=len(corpus), epochs=10)\n",
    "wv2 = dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating\n",
      "  Accuracy on train  is: 0.8107813182016587\n",
      "  Accuracy on dev  is: 0.8122270742358079\n"
     ]
    }
   ],
   "source": [
    "idf = idf_table(corpus)\n",
    "train_X = w2v_feature_matrix(train_data, idf, wv2)\n",
    "dev_X = w2v_feature_matrix(dev_data, idf, wv2)\n",
    "cls = train_classifier(train_X, sentiment.trainy, 1)\n",
    "\n",
    "print(\"\\nEvaluating\")\n",
    "evaluate(train_X, sentiment.trainy, cls, 'train')\n",
    "evaluate(dev_X, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self_training added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 1 : dev accuracy: 0.8122270742358079\n",
      "         train size:   4582\n",
      "loop 2 : dev accuracy: 0.8165938864628821\n",
      "         train size:   10889\n",
      "loop 3 : dev accuracy: 0.8144104803493449\n",
      "         train size:   13092\n",
      "loop 4 : dev accuracy: 0.8165938864628821\n",
      "         train size:   14435\n"
     ]
    }
   ],
   "source": [
    "cur_train_data = deepcopy(train_data)\n",
    "cur_trainy = deepcopy(sentiment.trainy)\n",
    " \n",
    "unlabel_data = deepcopy(unlabeled.data)\n",
    "testX = unlabeled.X\n",
    "pre_acc = 0\n",
    "\n",
    "index = 0\n",
    "while True:\n",
    "    index += 1\n",
    "    trainX = w2v_feature_matrix(cur_train_data, idf, wv2)\n",
    "    devX = w2v_feature_matrix(dev_data, idf, wv2)\n",
    "    cls = train_classifier(trainX, cur_trainy, 1)\n",
    "    \n",
    "    dev_yp = cls.predict(devX)\n",
    "    dev_acc = metrics.accuracy_score(sentiment.devy, dev_yp)\n",
    "    print(\"loop\", index, \": dev accuracy:\", dev_acc)\n",
    "    print(\"         train size:  \", len(cur_train_data))\n",
    "    if index == 4:\n",
    "        break\n",
    "    pre_acc = dev_acc\n",
    "        \n",
    "    testX = w2v_feature_matrix(test_data, idf, wv2)\n",
    "    test_yp = cls.predict(testX)\n",
    "    test_confidence = abs(cls.decision_function(testX))\n",
    "    \n",
    "    expand_data = []\n",
    "    expand_y = []    \n",
    "    \n",
    "    for i in range(len(test_confidence)):\n",
    "        if test_confidence[i] > 4.5:\n",
    "            expand_data.append(test_data[i])\n",
    "            expand_y.append(test_yp[i])\n",
    "    \n",
    "    cur_train_data = train_data + expand_data\n",
    "    cur_trainy = np.concatenate((sentiment.trainy, np.array(expand_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictions to a file\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing predictions to a file\")\n",
    "unlabeled.X = w2v_feature_matrix(test_data, idf, wv2)\n",
    "write_pred_kaggle_file(unlabeled, cls, \"data/sentiment-pred.csv\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
